{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "play_list = [\"AAPL\", \"SPY\", \"^VIX\", \"^SOX\"]\n",
    "\n",
    "df = yf.download(\"AAPL\", period=\"10y\", interval=\"1d\", ignore_tz=True)\n",
    "df.to_csv(\"aapl_data.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import talib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "aapl_df = pd.read_csv(\"aapl_data.csv\")\n",
    "\n",
    "# Ensure the Date column is in datetime format\n",
    "if 'Date' in aapl_df.columns:\n",
    "    aapl_df['Date'] = pd.to_datetime(aapl_df['Date'])\n",
    "\n",
    "# Sort by date\n",
    "aapl_df = aapl_df.sort_values(by='Date', ascending=True) if 'Date' in aapl_df.columns else aapl_df\n",
    "\n",
    "### 1. Candlestick Pattern Recognition ###\n",
    "patterns = {\n",
    "    \"Doji\": talib.CDLDOJI,\n",
    "    \"Engulfing\": talib.CDLENGULFING,\n",
    "    \"Hammer\": talib.CDLHAMMER,\n",
    "    \"Morning Star\": talib.CDLMORNINGSTAR,\n",
    "    \"Evening Star\": talib.CDLEVENINGSTAR,\n",
    "}\n",
    "\n",
    "for pattern_name, pattern_func in patterns.items():\n",
    "    aapl_df[pattern_name] = pattern_func(aapl_df['Open'], aapl_df['High'], aapl_df['Low'], aapl_df['Close']) / 100\n",
    "\n",
    "### 2. Technical Indicators ###\n",
    "aapl_df['SMA_10'] = talib.SMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['SMA_50'] = talib.SMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['EMA_10'] = talib.EMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['EMA_50'] = talib.EMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['RSI_14'] = talib.RSI(aapl_df['Close'], timeperiod=14)\n",
    "aapl_df['MACD'], aapl_df['MACD_Signal'], aapl_df['MACD_Hist'] = talib.MACD(aapl_df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "# ### 3. Normalization ###\n",
    "# price_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'SMA_10', 'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "# aapl_df[price_columns] = (aapl_df[price_columns] - aapl_df[price_columns].min()) / (aapl_df[price_columns].max() - aapl_df[price_columns].min())\n",
    "\n",
    "# ### 4. Windowed Representation ###\n",
    "# window_size = 3  \n",
    "# feature_columns = ['Close', 'Volume', 'SMA_10', 'SMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "\n",
    "# for col in feature_columns:\n",
    "#     for i in range(1, window_size + 1):\n",
    "#         aapl_df[f\"{col}_lag{i}\"] = aapl_df[col].shift(i)\n",
    "\n",
    "# aapl_df = aapl_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Save the processed data\n",
    "aapl_df.to_csv(\"processed_aapl_data.csv\", index=False)\n",
    "\n",
    "print(\"Feature engineering completed! Processed data saved as 'processed_aapl_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, initial_balance=10000, trading_fee=0.001, window_size=20):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        \n",
    "        # Load market data\n",
    "        self.df = df.copy()\n",
    "        self.initial_balance = initial_balance\n",
    "        self.trading_fee = trading_fee\n",
    "        self.window_size = window_size\n",
    "        self.done = False\n",
    "        \n",
    "        # Define state space (features from the dataset)\n",
    "        self.feature_columns = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "            'Engulfing', 'Hammer', 'Morning Star', 'Evening Star', 'SMA_10',\n",
    "            'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal',\n",
    "            'MACD_Hist'\n",
    "        ]\n",
    "        self.state_size = len(self.feature_columns)\n",
    "        \n",
    "        # Define action space (Buy, Sell, Hold)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.state_size,), dtype=np.float32)\n",
    "        \n",
    "        # Portfolio state variables\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = initial_balance\n",
    "        self.next_portfolio_value = initial_balance\n",
    "        self.returns = []\n",
    "        self.actions = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.done = False\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.returns = []\n",
    "        \n",
    "        return self._next_observation()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "# Load the processed AAPL stock data\n",
    "processed_aapl_df = pd.read_csv(\"processed_aapl_data.csv\")\n",
    "\n",
    "# Ensure Date is in datetime format\n",
    "if 'Date' in processed_aapl_df.columns:\n",
    "    processed_aapl_df['Date'] = pd.to_datetime(processed_aapl_df['Date'])\n",
    "\n",
    "# Sort the dataset\n",
    "processed_aapl_df = processed_aapl_df.sort_values(by='Date', ascending=True)\n",
    "\n",
    "\n",
    "# Initialize the environment\n",
    "env = StockTradingEnv(processed_aapl_df)\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Run a small test episode (10 steps)\n",
    "num_steps = 10\n",
    "print(\"\\n--- Running Test Episode ---\\n\")\n",
    "for _ in range(num_steps):\n",
    "    action = env.action_space.sample()  # Take a random action (Buy, Sell, Hold)\n",
    "    next_state, reward, done, _ = env.step(action)  # Step the environment\n",
    "    print(f\"action: {action}\")\n",
    "    env.render()  # Print current portfolio state\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode ended early.\")\n",
    "        break  # Stop if episode is done\n",
    "\n",
    "print(\"\\n--- Test Completed ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_state(df, current_step, window_size=20):\n",
    "    df_window = df[current_step - window_size:current_step].copy()\n",
    "    norm_cols = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "        'SMA_10', 'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD',\n",
    "        'MACD_Signal', 'MACD_Hist'\n",
    "    ]\n",
    "    df_copy = df_window[norm_cols].copy()\n",
    "    df_window[norm_cols] = (df_copy - df_copy.min()) / (df_copy.max() - df_copy.min())\n",
    "\n",
    "    op_cols = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "        'Engulfing', 'Hammer', 'Morning Star', 'Evening Star', 'SMA_10',\n",
    "        'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal',\n",
    "        'MACD_Hist'\n",
    "    ]\n",
    "    df_tensor = torch.tensor(df_window[op_cols].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    return df_tensor\n",
    "\n",
    "# Define the Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, window_size=20):\n",
    "        super(DQN, self).__init__()\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_dim * window_size, 128)  # Adjusted for 20-day window\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flat1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, window_size=20, gamma=0.99, lr=0.001, batch_size=32, memory_size=10000):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        self.model = DQN(state_size, action_size, window_size=window_size)\n",
    "        self.target_model = DQN(state_size, action_size, window_size=window_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Sync target model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def select_action(self, state, epsilon=0.1):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)  # Random action\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.model(state)).item()  # Best action from Q-network\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using experience replay.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "        \n",
    "        loss = self.criterion(q_values, target_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Sync the target model with the main model.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: 85.40, Epsilon: 0.9950\n",
      "Episode 2/500, Total Reward: 55.48, Epsilon: 0.9900\n",
      "Episode 3/500, Total Reward: -52.44, Epsilon: 0.9851\n",
      "Episode 4/500, Total Reward: 121.27, Epsilon: 0.9801\n",
      "Episode 5/500, Total Reward: 42.65, Epsilon: 0.9752\n",
      "Episode 6/500, Total Reward: 42.02, Epsilon: 0.9704\n",
      "Episode 7/500, Total Reward: 40.70, Epsilon: 0.9655\n",
      "Episode 8/500, Total Reward: -34.27, Epsilon: 0.9607\n",
      "Episode 9/500, Total Reward: 28.34, Epsilon: 0.9559\n",
      "Episode 10/500, Total Reward: 96.01, Epsilon: 0.9511\n",
      "Episode 11/500, Total Reward: 40.94, Epsilon: 0.9464\n",
      "Episode 12/500, Total Reward: 57.89, Epsilon: 0.9416\n",
      "Episode 13/500, Total Reward: -15.87, Epsilon: 0.9369\n",
      "Episode 14/500, Total Reward: 73.71, Epsilon: 0.9322\n",
      "Episode 15/500, Total Reward: 74.44, Epsilon: 0.9276\n",
      "Episode 16/500, Total Reward: 69.68, Epsilon: 0.9229\n",
      "Episode 17/500, Total Reward: 42.87, Epsilon: 0.9183\n",
      "Episode 18/500, Total Reward: -24.58, Epsilon: 0.9137\n",
      "Episode 19/500, Total Reward: 11.20, Epsilon: 0.9092\n",
      "Episode 20/500, Total Reward: -36.47, Epsilon: 0.9046\n",
      "Episode 21/500, Total Reward: -3.12, Epsilon: 0.9001\n",
      "Episode 22/500, Total Reward: 7.91, Epsilon: 0.8956\n",
      "Episode 23/500, Total Reward: 26.32, Epsilon: 0.8911\n",
      "Episode 24/500, Total Reward: 40.31, Epsilon: 0.8867\n",
      "Episode 25/500, Total Reward: 36.86, Epsilon: 0.8822\n",
      "Episode 26/500, Total Reward: -5.66, Epsilon: 0.8778\n",
      "Episode 27/500, Total Reward: 105.93, Epsilon: 0.8734\n",
      "Episode 28/500, Total Reward: 21.06, Epsilon: 0.8691\n",
      "Episode 29/500, Total Reward: 54.11, Epsilon: 0.8647\n",
      "Episode 30/500, Total Reward: -55.87, Epsilon: 0.8604\n",
      "Episode 31/500, Total Reward: 21.31, Epsilon: 0.8561\n",
      "Episode 32/500, Total Reward: 84.05, Epsilon: 0.8518\n",
      "Episode 33/500, Total Reward: 19.76, Epsilon: 0.8475\n",
      "Episode 34/500, Total Reward: 65.10, Epsilon: 0.8433\n",
      "Episode 35/500, Total Reward: 22.91, Epsilon: 0.8391\n",
      "Episode 36/500, Total Reward: 49.96, Epsilon: 0.8349\n",
      "Episode 37/500, Total Reward: -38.35, Epsilon: 0.8307\n",
      "Episode 38/500, Total Reward: 147.52, Epsilon: 0.8266\n",
      "Episode 39/500, Total Reward: -14.75, Epsilon: 0.8224\n",
      "Episode 40/500, Total Reward: -16.57, Epsilon: 0.8183\n",
      "Episode 41/500, Total Reward: 42.06, Epsilon: 0.8142\n",
      "Episode 42/500, Total Reward: -25.84, Epsilon: 0.8102\n",
      "Episode 43/500, Total Reward: 38.34, Epsilon: 0.8061\n",
      "Episode 44/500, Total Reward: -55.80, Epsilon: 0.8021\n",
      "Episode 45/500, Total Reward: 47.62, Epsilon: 0.7981\n",
      "Episode 46/500, Total Reward: 154.74, Epsilon: 0.7941\n",
      "Episode 47/500, Total Reward: 69.25, Epsilon: 0.7901\n",
      "Episode 48/500, Total Reward: 78.29, Epsilon: 0.7862\n",
      "Episode 49/500, Total Reward: 15.38, Epsilon: 0.7822\n",
      "Episode 50/500, Total Reward: 7.73, Epsilon: 0.7783\n",
      "Episode 51/500, Total Reward: 104.73, Epsilon: 0.7744\n",
      "Episode 52/500, Total Reward: 79.53, Epsilon: 0.7705\n",
      "Episode 53/500, Total Reward: 19.09, Epsilon: 0.7667\n",
      "Episode 54/500, Total Reward: 93.55, Epsilon: 0.7629\n",
      "Episode 55/500, Total Reward: 32.01, Epsilon: 0.7590\n",
      "Episode 56/500, Total Reward: 38.97, Epsilon: 0.7553\n",
      "Episode 57/500, Total Reward: -33.69, Epsilon: 0.7515\n",
      "Episode 58/500, Total Reward: 61.07, Epsilon: 0.7477\n",
      "Episode 59/500, Total Reward: 0.38, Epsilon: 0.7440\n",
      "Episode 60/500, Total Reward: 66.87, Epsilon: 0.7403\n",
      "Episode 61/500, Total Reward: -78.39, Epsilon: 0.7366\n",
      "Episode 62/500, Total Reward: 17.65, Epsilon: 0.7329\n",
      "Episode 63/500, Total Reward: -28.26, Epsilon: 0.7292\n",
      "Episode 64/500, Total Reward: -29.83, Epsilon: 0.7256\n",
      "Episode 65/500, Total Reward: 23.11, Epsilon: 0.7219\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(\"processed_aapl_data.csv\")\n",
    "\n",
    "env = StockTradingEnv(df)\n",
    "state_size = len(env.feature_columns)\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "episodes = 500  # Number of training episodes\n",
    "batch_size = 32  # Batch size for experience replay\n",
    "epsilon_decay = 0.995  # Decay factor for epsilon-greedy exploration\n",
    "min_epsilon = 0.01  # Minimum exploration probability\n",
    "update_target_every = 10  # Frequency to update target network\n",
    "\n",
    "epsilon = 1.0  # Initial exploration probability\n",
    "\n",
    "def train():\n",
    "    global epsilon\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = preprocess_state(df, env.current_step)  # Preprocess initial state\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state.unsqueeze(0), epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess_state(df, env.current_step)\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        if episode % update_target_every == 0:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    torch.save(agent.model.state_dict(), \"dqn_trading_model.pth\")  # Save trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random.sample(agent.memory, agent.batch_size)\n",
    "states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "states = torch.stack(states)\n",
    "actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "next_states = torch.stack(next_states)\n",
    "dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "q_values = agent.model(states).gather(1, actions)\n",
    "next_q_values = agent.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "target_q_values = rewards + (agent.gamma * next_q_values * (1 - dones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoreq312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
