{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "play_list = [\"AAPL\", \"SPY\", \"^VIX\", \"^SOX\"]\n",
    "\n",
    "df = yf.download(\"AAPL\", period=\"10y\", interval=\"1d\", ignore_tz=True)\n",
    "df.to_csv(\"aapl_data.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import talib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "aapl_df = pd.read_csv(\"aapl_data.csv\")\n",
    "\n",
    "# Ensure the Date column is in datetime format\n",
    "if 'Date' in aapl_df.columns:\n",
    "    aapl_df['Date'] = pd.to_datetime(aapl_df['Date'])\n",
    "\n",
    "# Sort by date\n",
    "aapl_df = aapl_df.sort_values(by='Date', ascending=True) if 'Date' in aapl_df.columns else aapl_df\n",
    "\n",
    "### 1. Candlestick Pattern Recognition ###\n",
    "patterns = {\n",
    "    \"Doji\": talib.CDLDOJI,\n",
    "    \"Engulfing\": talib.CDLENGULFING,\n",
    "    \"Hammer\": talib.CDLHAMMER,\n",
    "    \"Morning Star\": talib.CDLMORNINGSTAR,\n",
    "    \"Evening Star\": talib.CDLEVENINGSTAR,\n",
    "}\n",
    "\n",
    "for pattern_name, pattern_func in patterns.items():\n",
    "    aapl_df[pattern_name] = pattern_func(aapl_df['Open'], aapl_df['High'], aapl_df['Low'], aapl_df['Close']) / 100\n",
    "\n",
    "### 2. Technical Indicators ###\n",
    "aapl_df['SMA_10'] = talib.SMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['SMA_50'] = talib.SMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['EMA_10'] = talib.EMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['EMA_50'] = talib.EMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['RSI_14'] = talib.RSI(aapl_df['Close'], timeperiod=14)\n",
    "aapl_df['MACD'], aapl_df['MACD_Signal'], aapl_df['MACD_Hist'] = talib.MACD(aapl_df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "# ### 3. Normalization ###\n",
    "# price_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'SMA_10', 'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "# aapl_df[price_columns] = (aapl_df[price_columns] - aapl_df[price_columns].min()) / (aapl_df[price_columns].max() - aapl_df[price_columns].min())\n",
    "\n",
    "# ### 4. Windowed Representation ###\n",
    "# window_size = 3  \n",
    "# feature_columns = ['Close', 'Volume', 'SMA_10', 'SMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "\n",
    "# for col in feature_columns:\n",
    "#     for i in range(1, window_size + 1):\n",
    "#         aapl_df[f\"{col}_lag{i}\"] = aapl_df[col].shift(i)\n",
    "\n",
    "# aapl_df = aapl_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Save the processed data\n",
    "aapl_df.to_csv(\"processed_aapl_data.csv\", index=False)\n",
    "\n",
    "print(\"Feature engineering completed! Processed data saved as 'processed_aapl_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, initial_balance=10000, trading_fee=0.001, window_size=10, start_step=70):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        \n",
    "        # Load market data\n",
    "        self.df = df.copy()\n",
    "        self.initial_balance = initial_balance\n",
    "        self.trading_fee = trading_fee\n",
    "        self.window_size = window_size\n",
    "        self.current_step = start_step\n",
    "        self.start_step = start_step\n",
    "        self.done = False\n",
    "        \n",
    "        # Define state space (features from the dataset)\n",
    "        self.feature_columns = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "            'Engulfing', 'Hammer', 'Morning Star', 'Evening Star', 'SMA_10',\n",
    "            'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal',\n",
    "            'MACD_Hist'\n",
    "        ]\n",
    "        self.state_size = len(self.feature_columns)\n",
    "        \n",
    "        # Define action space (Buy, Sell, Hold)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.state_size,), dtype=np.float32)\n",
    "        \n",
    "        # Portfolio state variables\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = initial_balance\n",
    "        self.next_portfolio_value = initial_balance\n",
    "        self.returns = []\n",
    "        self.actions = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.current_step = self.start_step\n",
    "        self.done = False\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.returns = []\n",
    "        \n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        \"\"\"Returns the current market state as a feature vector.\"\"\"\n",
    "        return np.array(self.df.iloc[self.current_step][[col for col in self.feature_columns]], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executes the given action and moves the environment forward.\"\"\"\n",
    "        current_price = self.df.iloc[self.current_step]['Close']\n",
    "        next_price = self.df.iloc[self.current_step + 1]['Close']\n",
    "        \n",
    "        if action == 1:  # Buy\n",
    "            if self.balance > 0.001:\n",
    "                shares_to_buy = self.balance / (current_price * (1 + self.trading_fee))\n",
    "                self.shares_held = shares_to_buy\n",
    "                self.balance -= shares_to_buy * current_price * (1 + self.trading_fee)\n",
    "        elif action == 2:  # Sell\n",
    "            if self.shares_held > 0.001:\n",
    "                self.balance += self.shares_held * current_price * (1 - self.trading_fee)\n",
    "                self.shares_held = 0\n",
    "        \n",
    "        self.portfolio_value = self.balance + (self.shares_held * current_price)\n",
    "        self.next_portfolio_value = self.balance + (self.shares_held * next_price)\n",
    "        \n",
    "        # Compute returns\n",
    "        self.returns.append((self.next_portfolio_value - self.portfolio_value) / self.portfolio_value)\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        # Compute Sharpe Ratio (risk-adjusted reward)\n",
    "        if len(self.returns) > 1:\n",
    "            mean_return = np.mean(self.returns)\n",
    "            std_return = np.std(self.returns) if np.std(self.returns) > 0 else 1\n",
    "            sharpe_ratio = mean_return / std_return\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "        \n",
    "        reward = sharpe_ratio\n",
    "        \n",
    "        self.portfolio_value = self.next_portfolio_value\n",
    "\n",
    "        # Move to the next step\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            self.done = True\n",
    "        \n",
    "        return self._next_observation(), reward, self.done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Displays the current portfolio state.\"\"\"\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance:.2f}, Shares Held: {self.shares_held}, Portfolio Value: {self.portfolio_value}, Sharpe Ratio: {reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Test Episode ---\n",
      "\n",
      "action: 2\n",
      "Step: 71, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0\n",
      "action: 2\n",
      "Step: 72, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 73, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 74, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 75, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 76, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 77, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 78, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 79, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "action: 2\n",
      "Step: 80, Balance: 10000.00, Shares Held: 0, Portfolio Value: 10000.0, Sharpe Ratio: 0.0\n",
      "\n",
      "--- Test Completed ---\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "# Load the processed AAPL stock data\n",
    "processed_aapl_df = pd.read_csv(\"processed_aapl_data.csv\")\n",
    "\n",
    "# Ensure Date is in datetime format\n",
    "if 'Date' in processed_aapl_df.columns:\n",
    "    processed_aapl_df['Date'] = pd.to_datetime(processed_aapl_df['Date'])\n",
    "\n",
    "# Sort the dataset\n",
    "processed_aapl_df = processed_aapl_df.sort_values(by='Date', ascending=True)\n",
    "\n",
    "\n",
    "# Initialize the environment\n",
    "env = StockTradingEnv(processed_aapl_df)\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()\n",
    "\n",
    "# Run a small test episode (10 steps)\n",
    "num_steps = 10\n",
    "print(\"\\n--- Running Test Episode ---\\n\")\n",
    "for _ in range(num_steps):\n",
    "    action = env.action_space.sample()  # Take a random action (Buy, Sell, Hold)\n",
    "    next_state, reward, done, _ = env.step(action)  # Step the environment\n",
    "    print(f\"action: {action}\")\n",
    "    env.render()  # Print current portfolio state\n",
    "    \n",
    "    if done:\n",
    "        print(\"Episode ended early.\")\n",
    "        break  # Stop if episode is done\n",
    "\n",
    "print(\"\\n--- Test Completed ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_state(df, current_step, window_size=20):\n",
    "    df_window = df[current_step - window_size:current_step].copy()\n",
    "    norm_cols = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "        'SMA_10', 'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD',\n",
    "        'MACD_Signal', 'MACD_Hist'\n",
    "    ]\n",
    "    df_copy = df_window[norm_cols].copy()\n",
    "    df_window[norm_cols] = (df_copy - df_copy.min()) / (df_copy.max() - df_copy.min())\n",
    "\n",
    "    op_cols = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "        'Engulfing', 'Hammer', 'Morning Star', 'Evening Star', 'SMA_10',\n",
    "        'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal',\n",
    "        'MACD_Hist'\n",
    "    ]\n",
    "    df_tensor = torch.tensor(df_window[op_cols].to_numpy(), dtype=torch.float32)\n",
    "\n",
    "    return df_tensor\n",
    "\n",
    "# Define the Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, window_size=20):\n",
    "        super(DQN, self).__init__()\n",
    "        self.flat1 = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(input_dim * window_size, 128)  # Adjusted for 20-day window\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flat1(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, window_size=20, gamma=0.99, lr=0.001, batch_size=32, memory_size=10000):\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        self.model = DQN(state_size, action_size, window_size=window_size)\n",
    "        self.target_model = DQN(state_size, action_size, window_size=window_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Sync target model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def select_action(self, state, epsilon=0.1):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)  # Random action\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.model(state)).item()  # Best action from Q-network\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using experience replay.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(agent.memory, agent.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        q_values = agent.model(states).gather(1, actions)\n",
    "        next_q_values = agent.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + (agent.gamma * next_q_values * (1 - dones))\n",
    "        \n",
    "        loss = self.criterion(q_values, target_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Sync the target model with the main model.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/500, Total Reward: 110.40, Epsilon: 0.9950\n",
      "Episode 2/500, Total Reward: 127.65, Epsilon: 0.9900\n",
      "Episode 3/500, Total Reward: -21.91, Epsilon: 0.9851\n",
      "Episode 4/500, Total Reward: 17.82, Epsilon: 0.9801\n",
      "Episode 5/500, Total Reward: -22.41, Epsilon: 0.9752\n",
      "Episode 6/500, Total Reward: 60.34, Epsilon: 0.9704\n",
      "Episode 7/500, Total Reward: -16.41, Epsilon: 0.9655\n",
      "Episode 8/500, Total Reward: -25.14, Epsilon: 0.9607\n",
      "Episode 9/500, Total Reward: 61.39, Epsilon: 0.9559\n",
      "Episode 10/500, Total Reward: 103.03, Epsilon: 0.9511\n",
      "Episode 11/500, Total Reward: 62.98, Epsilon: 0.9464\n",
      "Episode 12/500, Total Reward: 59.77, Epsilon: 0.9416\n",
      "Episode 13/500, Total Reward: 11.52, Epsilon: 0.9369\n",
      "Episode 14/500, Total Reward: 124.32, Epsilon: 0.9322\n",
      "Episode 15/500, Total Reward: -25.63, Epsilon: 0.9276\n",
      "Episode 16/500, Total Reward: 2.38, Epsilon: 0.9229\n",
      "Episode 17/500, Total Reward: 102.37, Epsilon: 0.9183\n",
      "Episode 18/500, Total Reward: 23.02, Epsilon: 0.9137\n",
      "Episode 19/500, Total Reward: 53.70, Epsilon: 0.9092\n",
      "Episode 20/500, Total Reward: 88.08, Epsilon: 0.9046\n",
      "Episode 21/500, Total Reward: 71.06, Epsilon: 0.9001\n",
      "Episode 22/500, Total Reward: 171.89, Epsilon: 0.8956\n",
      "Episode 23/500, Total Reward: 202.06, Epsilon: 0.8911\n",
      "Episode 24/500, Total Reward: 62.19, Epsilon: 0.8867\n",
      "Episode 25/500, Total Reward: 119.08, Epsilon: 0.8822\n",
      "Episode 26/500, Total Reward: 38.74, Epsilon: 0.8778\n",
      "Episode 27/500, Total Reward: 10.25, Epsilon: 0.8734\n",
      "Episode 28/500, Total Reward: 13.97, Epsilon: 0.8691\n",
      "Episode 29/500, Total Reward: 15.58, Epsilon: 0.8647\n",
      "Episode 30/500, Total Reward: 40.35, Epsilon: 0.8604\n",
      "Episode 31/500, Total Reward: 88.64, Epsilon: 0.8561\n",
      "Episode 32/500, Total Reward: 58.85, Epsilon: 0.8518\n",
      "Episode 33/500, Total Reward: 76.86, Epsilon: 0.8475\n",
      "Episode 34/500, Total Reward: 143.54, Epsilon: 0.8433\n",
      "Episode 35/500, Total Reward: 22.88, Epsilon: 0.8391\n",
      "Episode 36/500, Total Reward: -26.49, Epsilon: 0.8349\n",
      "Episode 37/500, Total Reward: 9.25, Epsilon: 0.8307\n",
      "Episode 38/500, Total Reward: 69.92, Epsilon: 0.8266\n",
      "Episode 39/500, Total Reward: 12.86, Epsilon: 0.8224\n",
      "Episode 40/500, Total Reward: -4.13, Epsilon: 0.8183\n",
      "Episode 41/500, Total Reward: 88.39, Epsilon: 0.8142\n",
      "Episode 42/500, Total Reward: 34.22, Epsilon: 0.8102\n",
      "Episode 43/500, Total Reward: -34.37, Epsilon: 0.8061\n",
      "Episode 44/500, Total Reward: -8.83, Epsilon: 0.8021\n",
      "Episode 45/500, Total Reward: -17.71, Epsilon: 0.7981\n",
      "Episode 46/500, Total Reward: 40.38, Epsilon: 0.7941\n",
      "Episode 47/500, Total Reward: 31.38, Epsilon: 0.7901\n",
      "Episode 48/500, Total Reward: 2.05, Epsilon: 0.7862\n",
      "Episode 49/500, Total Reward: 9.07, Epsilon: 0.7822\n",
      "Episode 50/500, Total Reward: 135.31, Epsilon: 0.7783\n",
      "Episode 51/500, Total Reward: 98.61, Epsilon: 0.7744\n",
      "Episode 52/500, Total Reward: 50.57, Epsilon: 0.7705\n",
      "Episode 53/500, Total Reward: 81.72, Epsilon: 0.7667\n",
      "Episode 54/500, Total Reward: -1.10, Epsilon: 0.7629\n",
      "Episode 55/500, Total Reward: 3.40, Epsilon: 0.7590\n",
      "Episode 56/500, Total Reward: 37.12, Epsilon: 0.7553\n",
      "Episode 57/500, Total Reward: 1.55, Epsilon: 0.7515\n",
      "Episode 58/500, Total Reward: 200.68, Epsilon: 0.7477\n",
      "Episode 59/500, Total Reward: 32.29, Epsilon: 0.7440\n",
      "Episode 60/500, Total Reward: -55.64, Epsilon: 0.7403\n",
      "Episode 61/500, Total Reward: 84.58, Epsilon: 0.7366\n",
      "Episode 62/500, Total Reward: -4.67, Epsilon: 0.7329\n",
      "Episode 63/500, Total Reward: 3.31, Epsilon: 0.7292\n",
      "Episode 64/500, Total Reward: 24.05, Epsilon: 0.7256\n",
      "Episode 65/500, Total Reward: 24.35, Epsilon: 0.7219\n",
      "Episode 66/500, Total Reward: 42.48, Epsilon: 0.7183\n",
      "Episode 67/500, Total Reward: 48.69, Epsilon: 0.7147\n",
      "Episode 68/500, Total Reward: 12.42, Epsilon: 0.7112\n",
      "Episode 69/500, Total Reward: 44.97, Epsilon: 0.7076\n",
      "Episode 70/500, Total Reward: -2.39, Epsilon: 0.7041\n",
      "Episode 71/500, Total Reward: 59.01, Epsilon: 0.7005\n",
      "Episode 72/500, Total Reward: 63.89, Epsilon: 0.6970\n",
      "Episode 73/500, Total Reward: 10.92, Epsilon: 0.6936\n",
      "Episode 74/500, Total Reward: 78.03, Epsilon: 0.6901\n",
      "Episode 75/500, Total Reward: 47.14, Epsilon: 0.6866\n",
      "Episode 76/500, Total Reward: -8.13, Epsilon: 0.6832\n",
      "Episode 77/500, Total Reward: 100.69, Epsilon: 0.6798\n",
      "Episode 78/500, Total Reward: 117.36, Epsilon: 0.6764\n",
      "Episode 79/500, Total Reward: -42.26, Epsilon: 0.6730\n",
      "Episode 80/500, Total Reward: -3.40, Epsilon: 0.6696\n",
      "Episode 81/500, Total Reward: -65.45, Epsilon: 0.6663\n",
      "Episode 82/500, Total Reward: -7.35, Epsilon: 0.6630\n",
      "Episode 83/500, Total Reward: -44.50, Epsilon: 0.6597\n",
      "Episode 84/500, Total Reward: 118.29, Epsilon: 0.6564\n",
      "Episode 85/500, Total Reward: 14.50, Epsilon: 0.6531\n",
      "Episode 86/500, Total Reward: 155.70, Epsilon: 0.6498\n",
      "Episode 87/500, Total Reward: -29.86, Epsilon: 0.6466\n",
      "Episode 88/500, Total Reward: 47.50, Epsilon: 0.6433\n",
      "Episode 89/500, Total Reward: -0.49, Epsilon: 0.6401\n",
      "Episode 90/500, Total Reward: 52.53, Epsilon: 0.6369\n",
      "Episode 91/500, Total Reward: 36.30, Epsilon: 0.6337\n",
      "Episode 92/500, Total Reward: 17.71, Epsilon: 0.6306\n",
      "Episode 93/500, Total Reward: 89.97, Epsilon: 0.6274\n",
      "Episode 94/500, Total Reward: 48.68, Epsilon: 0.6243\n",
      "Episode 95/500, Total Reward: 27.94, Epsilon: 0.6211\n",
      "Episode 96/500, Total Reward: 0.23, Epsilon: 0.6180\n",
      "Episode 97/500, Total Reward: 84.82, Epsilon: 0.6149\n",
      "Episode 98/500, Total Reward: 91.39, Epsilon: 0.6119\n",
      "Episode 99/500, Total Reward: -9.72, Epsilon: 0.6088\n",
      "Episode 100/500, Total Reward: 53.05, Epsilon: 0.6058\n",
      "Episode 101/500, Total Reward: -17.21, Epsilon: 0.6027\n",
      "Episode 102/500, Total Reward: -41.90, Epsilon: 0.5997\n",
      "Episode 103/500, Total Reward: -19.47, Epsilon: 0.5967\n",
      "Episode 104/500, Total Reward: 18.42, Epsilon: 0.5937\n",
      "Episode 105/500, Total Reward: -17.43, Epsilon: 0.5908\n",
      "Episode 106/500, Total Reward: 18.61, Epsilon: 0.5878\n",
      "Episode 107/500, Total Reward: 92.23, Epsilon: 0.5849\n",
      "Episode 108/500, Total Reward: -29.23, Epsilon: 0.5820\n",
      "Episode 109/500, Total Reward: 120.50, Epsilon: 0.5790\n",
      "Episode 110/500, Total Reward: 62.91, Epsilon: 0.5762\n",
      "Episode 111/500, Total Reward: 2.57, Epsilon: 0.5733\n",
      "Episode 112/500, Total Reward: 74.27, Epsilon: 0.5704\n",
      "Episode 113/500, Total Reward: 22.02, Epsilon: 0.5676\n",
      "Episode 114/500, Total Reward: 68.62, Epsilon: 0.5647\n",
      "Episode 115/500, Total Reward: 28.00, Epsilon: 0.5619\n",
      "Episode 116/500, Total Reward: -13.03, Epsilon: 0.5591\n",
      "Episode 117/500, Total Reward: 83.17, Epsilon: 0.5563\n",
      "Episode 118/500, Total Reward: 78.87, Epsilon: 0.5535\n",
      "Episode 119/500, Total Reward: -18.98, Epsilon: 0.5507\n",
      "Episode 120/500, Total Reward: 106.54, Epsilon: 0.5480\n",
      "Episode 121/500, Total Reward: 9.16, Epsilon: 0.5452\n",
      "Episode 122/500, Total Reward: 28.52, Epsilon: 0.5425\n",
      "Episode 123/500, Total Reward: 4.89, Epsilon: 0.5398\n",
      "Episode 124/500, Total Reward: -74.06, Epsilon: 0.5371\n",
      "Episode 125/500, Total Reward: 22.90, Epsilon: 0.5344\n",
      "Episode 126/500, Total Reward: 39.35, Epsilon: 0.5318\n",
      "Episode 127/500, Total Reward: 13.12, Epsilon: 0.5291\n",
      "Episode 128/500, Total Reward: 1.88, Epsilon: 0.5264\n",
      "Episode 129/500, Total Reward: 89.28, Epsilon: 0.5238\n",
      "Episode 130/500, Total Reward: 24.65, Epsilon: 0.5212\n",
      "Episode 131/500, Total Reward: 57.99, Epsilon: 0.5186\n",
      "Episode 132/500, Total Reward: -12.36, Epsilon: 0.5160\n",
      "Episode 133/500, Total Reward: 141.01, Epsilon: 0.5134\n",
      "Episode 134/500, Total Reward: 68.77, Epsilon: 0.5108\n",
      "Episode 135/500, Total Reward: 9.20, Epsilon: 0.5083\n",
      "Episode 136/500, Total Reward: 45.87, Epsilon: 0.5058\n",
      "Episode 137/500, Total Reward: 44.88, Epsilon: 0.5032\n",
      "Episode 138/500, Total Reward: 77.39, Epsilon: 0.5007\n",
      "Episode 139/500, Total Reward: 56.02, Epsilon: 0.4982\n",
      "Episode 140/500, Total Reward: -2.83, Epsilon: 0.4957\n",
      "Episode 141/500, Total Reward: 94.43, Epsilon: 0.4932\n",
      "Episode 142/500, Total Reward: -4.50, Epsilon: 0.4908\n",
      "Episode 143/500, Total Reward: -5.26, Epsilon: 0.4883\n",
      "Episode 144/500, Total Reward: 74.13, Epsilon: 0.4859\n",
      "Episode 145/500, Total Reward: -18.89, Epsilon: 0.4834\n",
      "Episode 146/500, Total Reward: -23.06, Epsilon: 0.4810\n",
      "Episode 147/500, Total Reward: -3.70, Epsilon: 0.4786\n",
      "Episode 148/500, Total Reward: 81.69, Epsilon: 0.4762\n",
      "Episode 149/500, Total Reward: 99.76, Epsilon: 0.4738\n",
      "Episode 150/500, Total Reward: 54.50, Epsilon: 0.4715\n",
      "Episode 151/500, Total Reward: -47.95, Epsilon: 0.4691\n",
      "Episode 152/500, Total Reward: 51.67, Epsilon: 0.4668\n",
      "Episode 153/500, Total Reward: 65.14, Epsilon: 0.4644\n",
      "Episode 154/500, Total Reward: 46.03, Epsilon: 0.4621\n",
      "Episode 155/500, Total Reward: 39.00, Epsilon: 0.4598\n",
      "Episode 156/500, Total Reward: 8.85, Epsilon: 0.4575\n",
      "Episode 157/500, Total Reward: -28.60, Epsilon: 0.4552\n",
      "Episode 158/500, Total Reward: 77.97, Epsilon: 0.4529\n",
      "Episode 159/500, Total Reward: -19.48, Epsilon: 0.4507\n",
      "Episode 160/500, Total Reward: 60.09, Epsilon: 0.4484\n",
      "Episode 161/500, Total Reward: 12.24, Epsilon: 0.4462\n",
      "Episode 162/500, Total Reward: -26.57, Epsilon: 0.4440\n",
      "Episode 163/500, Total Reward: 10.69, Epsilon: 0.4417\n",
      "Episode 164/500, Total Reward: 34.59, Epsilon: 0.4395\n",
      "Episode 165/500, Total Reward: 85.42, Epsilon: 0.4373\n",
      "Episode 166/500, Total Reward: 81.17, Epsilon: 0.4351\n",
      "Episode 167/500, Total Reward: 24.52, Epsilon: 0.4330\n",
      "Episode 168/500, Total Reward: -42.94, Epsilon: 0.4308\n",
      "Episode 169/500, Total Reward: -7.81, Epsilon: 0.4286\n",
      "Episode 170/500, Total Reward: 84.97, Epsilon: 0.4265\n",
      "Episode 171/500, Total Reward: 8.89, Epsilon: 0.4244\n",
      "Episode 172/500, Total Reward: 104.29, Epsilon: 0.4223\n",
      "Episode 173/500, Total Reward: -60.24, Epsilon: 0.4201\n",
      "Episode 174/500, Total Reward: 90.07, Epsilon: 0.4180\n",
      "Episode 175/500, Total Reward: -13.00, Epsilon: 0.4159\n",
      "Episode 176/500, Total Reward: 56.02, Epsilon: 0.4139\n",
      "Episode 177/500, Total Reward: 85.59, Epsilon: 0.4118\n",
      "Episode 178/500, Total Reward: 17.69, Epsilon: 0.4097\n",
      "Episode 179/500, Total Reward: 70.39, Epsilon: 0.4077\n",
      "Episode 180/500, Total Reward: 6.86, Epsilon: 0.4057\n",
      "Episode 181/500, Total Reward: 78.50, Epsilon: 0.4036\n",
      "Episode 182/500, Total Reward: 32.97, Epsilon: 0.4016\n",
      "Episode 183/500, Total Reward: -32.74, Epsilon: 0.3996\n",
      "Episode 184/500, Total Reward: -2.23, Epsilon: 0.3976\n",
      "Episode 185/500, Total Reward: -36.02, Epsilon: 0.3956\n",
      "Episode 186/500, Total Reward: 40.54, Epsilon: 0.3936\n",
      "Episode 187/500, Total Reward: 80.74, Epsilon: 0.3917\n",
      "Episode 188/500, Total Reward: 134.74, Epsilon: 0.3897\n",
      "Episode 189/500, Total Reward: 80.31, Epsilon: 0.3878\n",
      "Episode 190/500, Total Reward: 39.00, Epsilon: 0.3858\n",
      "Episode 191/500, Total Reward: 4.87, Epsilon: 0.3839\n",
      "Episode 192/500, Total Reward: 83.30, Epsilon: 0.3820\n",
      "Episode 193/500, Total Reward: 89.63, Epsilon: 0.3801\n",
      "Episode 194/500, Total Reward: 72.14, Epsilon: 0.3782\n",
      "Episode 195/500, Total Reward: 48.06, Epsilon: 0.3763\n",
      "Episode 196/500, Total Reward: 12.03, Epsilon: 0.3744\n",
      "Episode 197/500, Total Reward: 64.65, Epsilon: 0.3725\n",
      "Episode 198/500, Total Reward: 0.11, Epsilon: 0.3707\n",
      "Episode 199/500, Total Reward: 80.52, Epsilon: 0.3688\n",
      "Episode 200/500, Total Reward: 69.31, Epsilon: 0.3670\n",
      "Episode 201/500, Total Reward: 68.67, Epsilon: 0.3651\n",
      "Episode 202/500, Total Reward: 93.93, Epsilon: 0.3633\n",
      "Episode 203/500, Total Reward: -1.16, Epsilon: 0.3615\n",
      "Episode 204/500, Total Reward: 58.95, Epsilon: 0.3597\n",
      "Episode 205/500, Total Reward: 115.01, Epsilon: 0.3579\n",
      "Episode 206/500, Total Reward: 2.20, Epsilon: 0.3561\n",
      "Episode 207/500, Total Reward: 115.07, Epsilon: 0.3543\n",
      "Episode 208/500, Total Reward: 17.52, Epsilon: 0.3525\n",
      "Episode 209/500, Total Reward: -13.82, Epsilon: 0.3508\n",
      "Episode 210/500, Total Reward: 5.00, Epsilon: 0.3490\n",
      "Episode 211/500, Total Reward: 61.92, Epsilon: 0.3473\n",
      "Episode 212/500, Total Reward: -102.44, Epsilon: 0.3455\n",
      "Episode 213/500, Total Reward: 125.90, Epsilon: 0.3438\n",
      "Episode 214/500, Total Reward: -28.72, Epsilon: 0.3421\n",
      "Episode 215/500, Total Reward: 66.18, Epsilon: 0.3404\n",
      "Episode 216/500, Total Reward: 67.99, Epsilon: 0.3387\n",
      "Episode 217/500, Total Reward: 10.34, Epsilon: 0.3370\n",
      "Episode 218/500, Total Reward: 0.36, Epsilon: 0.3353\n",
      "Episode 219/500, Total Reward: -69.22, Epsilon: 0.3336\n",
      "Episode 220/500, Total Reward: 113.66, Epsilon: 0.3320\n",
      "Episode 221/500, Total Reward: 32.75, Epsilon: 0.3303\n",
      "Episode 222/500, Total Reward: 84.75, Epsilon: 0.3286\n",
      "Episode 223/500, Total Reward: 9.94, Epsilon: 0.3270\n",
      "Episode 224/500, Total Reward: 40.12, Epsilon: 0.3254\n",
      "Episode 225/500, Total Reward: -2.34, Epsilon: 0.3237\n",
      "Episode 226/500, Total Reward: 77.92, Epsilon: 0.3221\n",
      "Episode 227/500, Total Reward: 79.43, Epsilon: 0.3205\n",
      "Episode 228/500, Total Reward: 98.16, Epsilon: 0.3189\n",
      "Episode 229/500, Total Reward: 22.89, Epsilon: 0.3173\n",
      "Episode 230/500, Total Reward: 56.42, Epsilon: 0.3157\n",
      "Episode 231/500, Total Reward: 23.71, Epsilon: 0.3141\n",
      "Episode 232/500, Total Reward: -38.08, Epsilon: 0.3126\n",
      "Episode 233/500, Total Reward: 53.78, Epsilon: 0.3110\n",
      "Episode 234/500, Total Reward: 21.08, Epsilon: 0.3095\n",
      "Episode 235/500, Total Reward: 18.79, Epsilon: 0.3079\n",
      "Episode 236/500, Total Reward: 60.40, Epsilon: 0.3064\n",
      "Episode 237/500, Total Reward: 2.50, Epsilon: 0.3048\n",
      "Episode 238/500, Total Reward: 37.01, Epsilon: 0.3033\n",
      "Episode 239/500, Total Reward: -14.78, Epsilon: 0.3018\n",
      "Episode 240/500, Total Reward: 99.94, Epsilon: 0.3003\n",
      "Episode 241/500, Total Reward: 57.44, Epsilon: 0.2988\n",
      "Episode 242/500, Total Reward: 4.81, Epsilon: 0.2973\n",
      "Episode 243/500, Total Reward: 16.64, Epsilon: 0.2958\n",
      "Episode 244/500, Total Reward: 25.03, Epsilon: 0.2943\n",
      "Episode 245/500, Total Reward: -68.06, Epsilon: 0.2929\n",
      "Episode 246/500, Total Reward: -0.61, Epsilon: 0.2914\n",
      "Episode 247/500, Total Reward: 48.50, Epsilon: 0.2899\n",
      "Episode 248/500, Total Reward: 93.96, Epsilon: 0.2885\n",
      "Episode 249/500, Total Reward: 34.47, Epsilon: 0.2870\n",
      "Episode 250/500, Total Reward: -15.17, Epsilon: 0.2856\n",
      "Episode 251/500, Total Reward: 92.75, Epsilon: 0.2842\n",
      "Episode 252/500, Total Reward: -108.34, Epsilon: 0.2828\n",
      "Episode 253/500, Total Reward: 49.04, Epsilon: 0.2813\n",
      "Episode 254/500, Total Reward: -15.16, Epsilon: 0.2799\n",
      "Episode 255/500, Total Reward: 77.84, Epsilon: 0.2785\n",
      "Episode 256/500, Total Reward: -2.86, Epsilon: 0.2771\n",
      "Episode 257/500, Total Reward: -52.08, Epsilon: 0.2758\n",
      "Episode 258/500, Total Reward: 69.43, Epsilon: 0.2744\n",
      "Episode 259/500, Total Reward: 51.00, Epsilon: 0.2730\n",
      "Episode 260/500, Total Reward: 96.38, Epsilon: 0.2716\n",
      "Episode 261/500, Total Reward: 55.18, Epsilon: 0.2703\n",
      "Episode 262/500, Total Reward: 51.43, Epsilon: 0.2689\n",
      "Episode 263/500, Total Reward: 26.11, Epsilon: 0.2676\n",
      "Episode 264/500, Total Reward: 61.05, Epsilon: 0.2663\n",
      "Episode 265/500, Total Reward: 57.92, Epsilon: 0.2649\n",
      "Episode 266/500, Total Reward: 158.30, Epsilon: 0.2636\n",
      "Episode 267/500, Total Reward: 46.62, Epsilon: 0.2623\n",
      "Episode 268/500, Total Reward: 47.83, Epsilon: 0.2610\n",
      "Episode 269/500, Total Reward: 114.05, Epsilon: 0.2597\n",
      "Episode 270/500, Total Reward: 21.59, Epsilon: 0.2584\n",
      "Episode 271/500, Total Reward: 50.16, Epsilon: 0.2571\n",
      "Episode 272/500, Total Reward: -3.98, Epsilon: 0.2558\n",
      "Episode 273/500, Total Reward: 76.59, Epsilon: 0.2545\n",
      "Episode 274/500, Total Reward: 117.15, Epsilon: 0.2532\n",
      "Episode 275/500, Total Reward: 65.43, Epsilon: 0.2520\n",
      "Episode 276/500, Total Reward: 76.37, Epsilon: 0.2507\n",
      "Episode 277/500, Total Reward: 112.48, Epsilon: 0.2495\n",
      "Episode 278/500, Total Reward: 57.81, Epsilon: 0.2482\n",
      "Episode 279/500, Total Reward: -18.11, Epsilon: 0.2470\n",
      "Episode 280/500, Total Reward: 78.51, Epsilon: 0.2457\n",
      "Episode 281/500, Total Reward: -17.79, Epsilon: 0.2445\n",
      "Episode 282/500, Total Reward: 80.28, Epsilon: 0.2433\n",
      "Episode 283/500, Total Reward: 9.93, Epsilon: 0.2421\n",
      "Episode 284/500, Total Reward: 73.76, Epsilon: 0.2409\n",
      "Episode 285/500, Total Reward: 19.88, Epsilon: 0.2397\n",
      "Episode 286/500, Total Reward: -59.55, Epsilon: 0.2385\n",
      "Episode 287/500, Total Reward: 129.83, Epsilon: 0.2373\n",
      "Episode 288/500, Total Reward: 103.03, Epsilon: 0.2361\n",
      "Episode 289/500, Total Reward: -16.73, Epsilon: 0.2349\n",
      "Episode 290/500, Total Reward: -2.47, Epsilon: 0.2337\n",
      "Episode 291/500, Total Reward: 56.83, Epsilon: 0.2326\n",
      "Episode 292/500, Total Reward: 38.90, Epsilon: 0.2314\n",
      "Episode 293/500, Total Reward: 32.73, Epsilon: 0.2302\n",
      "Episode 294/500, Total Reward: 96.04, Epsilon: 0.2291\n",
      "Episode 295/500, Total Reward: 39.95, Epsilon: 0.2279\n",
      "Episode 296/500, Total Reward: 21.88, Epsilon: 0.2268\n",
      "Episode 297/500, Total Reward: 65.83, Epsilon: 0.2257\n",
      "Episode 298/500, Total Reward: 104.18, Epsilon: 0.2245\n",
      "Episode 299/500, Total Reward: 59.71, Epsilon: 0.2234\n",
      "Episode 300/500, Total Reward: 13.19, Epsilon: 0.2223\n",
      "Episode 301/500, Total Reward: 28.07, Epsilon: 0.2212\n",
      "Episode 302/500, Total Reward: -6.32, Epsilon: 0.2201\n",
      "Episode 303/500, Total Reward: 17.97, Epsilon: 0.2190\n",
      "Episode 304/500, Total Reward: 4.36, Epsilon: 0.2179\n",
      "Episode 305/500, Total Reward: 27.66, Epsilon: 0.2168\n",
      "Episode 306/500, Total Reward: 13.98, Epsilon: 0.2157\n",
      "Episode 307/500, Total Reward: 132.55, Epsilon: 0.2146\n",
      "Episode 308/500, Total Reward: 38.50, Epsilon: 0.2136\n",
      "Episode 309/500, Total Reward: 43.18, Epsilon: 0.2125\n",
      "Episode 310/500, Total Reward: -28.39, Epsilon: 0.2114\n",
      "Episode 311/500, Total Reward: 122.90, Epsilon: 0.2104\n",
      "Episode 312/500, Total Reward: 15.04, Epsilon: 0.2093\n",
      "Episode 313/500, Total Reward: 53.12, Epsilon: 0.2083\n",
      "Episode 314/500, Total Reward: 83.43, Epsilon: 0.2072\n",
      "Episode 315/500, Total Reward: 125.37, Epsilon: 0.2062\n",
      "Episode 316/500, Total Reward: 94.17, Epsilon: 0.2052\n",
      "Episode 317/500, Total Reward: 173.19, Epsilon: 0.2041\n",
      "Episode 318/500, Total Reward: 28.33, Epsilon: 0.2031\n",
      "Episode 319/500, Total Reward: 73.32, Epsilon: 0.2021\n",
      "Episode 320/500, Total Reward: 107.35, Epsilon: 0.2011\n",
      "Episode 321/500, Total Reward: 20.47, Epsilon: 0.2001\n",
      "Episode 322/500, Total Reward: 107.36, Epsilon: 0.1991\n",
      "Episode 323/500, Total Reward: 78.33, Epsilon: 0.1981\n",
      "Episode 324/500, Total Reward: 25.57, Epsilon: 0.1971\n",
      "Episode 325/500, Total Reward: 45.30, Epsilon: 0.1961\n",
      "Episode 326/500, Total Reward: 13.67, Epsilon: 0.1951\n",
      "Episode 327/500, Total Reward: 132.62, Epsilon: 0.1942\n",
      "Episode 328/500, Total Reward: 103.45, Epsilon: 0.1932\n",
      "Episode 329/500, Total Reward: -34.43, Epsilon: 0.1922\n",
      "Episode 330/500, Total Reward: 33.66, Epsilon: 0.1913\n",
      "Episode 331/500, Total Reward: -46.33, Epsilon: 0.1903\n",
      "Episode 332/500, Total Reward: 82.54, Epsilon: 0.1893\n",
      "Episode 333/500, Total Reward: 17.83, Epsilon: 0.1884\n",
      "Episode 334/500, Total Reward: 84.32, Epsilon: 0.1875\n",
      "Episode 335/500, Total Reward: 15.12, Epsilon: 0.1865\n",
      "Episode 336/500, Total Reward: 21.66, Epsilon: 0.1856\n",
      "Episode 337/500, Total Reward: 129.93, Epsilon: 0.1847\n",
      "Episode 338/500, Total Reward: 70.48, Epsilon: 0.1837\n",
      "Episode 339/500, Total Reward: 76.44, Epsilon: 0.1828\n",
      "Episode 340/500, Total Reward: 97.40, Epsilon: 0.1819\n",
      "Episode 341/500, Total Reward: 72.81, Epsilon: 0.1810\n",
      "Episode 342/500, Total Reward: 59.36, Epsilon: 0.1801\n",
      "Episode 343/500, Total Reward: 48.94, Epsilon: 0.1792\n",
      "Episode 344/500, Total Reward: 3.80, Epsilon: 0.1783\n",
      "Episode 345/500, Total Reward: 67.78, Epsilon: 0.1774\n",
      "Episode 346/500, Total Reward: 84.70, Epsilon: 0.1765\n",
      "Episode 347/500, Total Reward: 126.35, Epsilon: 0.1756\n",
      "Episode 348/500, Total Reward: 71.58, Epsilon: 0.1748\n",
      "Episode 349/500, Total Reward: 55.13, Epsilon: 0.1739\n",
      "Episode 350/500, Total Reward: 79.64, Epsilon: 0.1730\n",
      "Episode 351/500, Total Reward: 43.16, Epsilon: 0.1721\n",
      "Episode 352/500, Total Reward: 113.94, Epsilon: 0.1713\n",
      "Episode 353/500, Total Reward: -7.40, Epsilon: 0.1704\n",
      "Episode 354/500, Total Reward: 2.11, Epsilon: 0.1696\n",
      "Episode 355/500, Total Reward: -20.84, Epsilon: 0.1687\n",
      "Episode 356/500, Total Reward: 55.25, Epsilon: 0.1679\n",
      "Episode 357/500, Total Reward: 4.29, Epsilon: 0.1670\n",
      "Episode 358/500, Total Reward: 1.37, Epsilon: 0.1662\n",
      "Episode 359/500, Total Reward: 45.31, Epsilon: 0.1654\n",
      "Episode 360/500, Total Reward: 44.69, Epsilon: 0.1646\n",
      "Episode 361/500, Total Reward: 64.83, Epsilon: 0.1637\n",
      "Episode 362/500, Total Reward: 82.63, Epsilon: 0.1629\n",
      "Episode 363/500, Total Reward: 81.02, Epsilon: 0.1621\n",
      "Episode 364/500, Total Reward: 165.40, Epsilon: 0.1613\n",
      "Episode 365/500, Total Reward: 46.20, Epsilon: 0.1605\n",
      "Episode 366/500, Total Reward: 102.36, Epsilon: 0.1597\n",
      "Episode 367/500, Total Reward: -28.37, Epsilon: 0.1589\n",
      "Episode 368/500, Total Reward: 127.76, Epsilon: 0.1581\n",
      "Episode 369/500, Total Reward: 97.06, Epsilon: 0.1573\n",
      "Episode 370/500, Total Reward: -21.77, Epsilon: 0.1565\n",
      "Episode 371/500, Total Reward: 25.74, Epsilon: 0.1557\n",
      "Episode 372/500, Total Reward: 27.01, Epsilon: 0.1549\n",
      "Episode 373/500, Total Reward: 103.77, Epsilon: 0.1542\n",
      "Episode 374/500, Total Reward: 54.79, Epsilon: 0.1534\n",
      "Episode 375/500, Total Reward: 83.51, Epsilon: 0.1526\n",
      "Episode 376/500, Total Reward: 86.25, Epsilon: 0.1519\n",
      "Episode 377/500, Total Reward: 25.97, Epsilon: 0.1511\n",
      "Episode 378/500, Total Reward: -24.93, Epsilon: 0.1504\n",
      "Episode 379/500, Total Reward: 71.92, Epsilon: 0.1496\n",
      "Episode 380/500, Total Reward: 104.03, Epsilon: 0.1489\n",
      "Episode 381/500, Total Reward: 86.17, Epsilon: 0.1481\n",
      "Episode 382/500, Total Reward: 2.25, Epsilon: 0.1474\n",
      "Episode 383/500, Total Reward: 100.53, Epsilon: 0.1466\n",
      "Episode 384/500, Total Reward: 119.28, Epsilon: 0.1459\n",
      "Episode 385/500, Total Reward: 1.60, Epsilon: 0.1452\n",
      "Episode 386/500, Total Reward: -4.90, Epsilon: 0.1444\n",
      "Episode 387/500, Total Reward: 2.07, Epsilon: 0.1437\n",
      "Episode 388/500, Total Reward: 127.47, Epsilon: 0.1430\n",
      "Episode 389/500, Total Reward: -12.98, Epsilon: 0.1423\n",
      "Episode 390/500, Total Reward: 35.90, Epsilon: 0.1416\n",
      "Episode 391/500, Total Reward: -6.22, Epsilon: 0.1409\n",
      "Episode 392/500, Total Reward: 68.91, Epsilon: 0.1402\n",
      "Episode 393/500, Total Reward: 43.97, Epsilon: 0.1395\n",
      "Episode 394/500, Total Reward: 80.95, Epsilon: 0.1388\n",
      "Episode 395/500, Total Reward: -17.46, Epsilon: 0.1381\n",
      "Episode 396/500, Total Reward: 134.17, Epsilon: 0.1374\n",
      "Episode 397/500, Total Reward: 86.30, Epsilon: 0.1367\n",
      "Episode 398/500, Total Reward: -13.19, Epsilon: 0.1360\n",
      "Episode 399/500, Total Reward: 9.06, Epsilon: 0.1353\n",
      "Episode 400/500, Total Reward: -12.12, Epsilon: 0.1347\n",
      "Episode 401/500, Total Reward: 16.75, Epsilon: 0.1340\n",
      "Episode 402/500, Total Reward: 52.38, Epsilon: 0.1333\n",
      "Episode 403/500, Total Reward: 27.17, Epsilon: 0.1326\n",
      "Episode 404/500, Total Reward: 130.49, Epsilon: 0.1320\n",
      "Episode 405/500, Total Reward: 43.85, Epsilon: 0.1313\n",
      "Episode 406/500, Total Reward: 20.05, Epsilon: 0.1307\n",
      "Episode 407/500, Total Reward: 29.46, Epsilon: 0.1300\n",
      "Episode 408/500, Total Reward: 6.89, Epsilon: 0.1294\n",
      "Episode 409/500, Total Reward: -21.27, Epsilon: 0.1287\n",
      "Episode 410/500, Total Reward: 120.48, Epsilon: 0.1281\n",
      "Episode 411/500, Total Reward: 29.05, Epsilon: 0.1274\n",
      "Episode 412/500, Total Reward: 105.31, Epsilon: 0.1268\n",
      "Episode 413/500, Total Reward: -4.99, Epsilon: 0.1262\n",
      "Episode 414/500, Total Reward: 125.74, Epsilon: 0.1255\n",
      "Episode 415/500, Total Reward: 79.19, Epsilon: 0.1249\n",
      "Episode 416/500, Total Reward: 11.37, Epsilon: 0.1243\n",
      "Episode 417/500, Total Reward: 51.15, Epsilon: 0.1237\n",
      "Episode 418/500, Total Reward: -28.46, Epsilon: 0.1230\n",
      "Episode 419/500, Total Reward: 85.52, Epsilon: 0.1224\n",
      "Episode 420/500, Total Reward: 33.98, Epsilon: 0.1218\n",
      "Episode 421/500, Total Reward: 68.97, Epsilon: 0.1212\n",
      "Episode 422/500, Total Reward: 84.64, Epsilon: 0.1206\n",
      "Episode 423/500, Total Reward: -16.93, Epsilon: 0.1200\n",
      "Episode 424/500, Total Reward: 62.48, Epsilon: 0.1194\n",
      "Episode 425/500, Total Reward: 46.41, Epsilon: 0.1188\n",
      "Episode 426/500, Total Reward: 37.07, Epsilon: 0.1182\n",
      "Episode 427/500, Total Reward: 34.54, Epsilon: 0.1176\n",
      "Episode 428/500, Total Reward: -20.88, Epsilon: 0.1170\n",
      "Episode 429/500, Total Reward: 29.57, Epsilon: 0.1164\n",
      "Episode 430/500, Total Reward: 79.51, Epsilon: 0.1159\n",
      "Episode 431/500, Total Reward: -4.07, Epsilon: 0.1153\n",
      "Episode 432/500, Total Reward: 61.70, Epsilon: 0.1147\n",
      "Episode 433/500, Total Reward: 98.28, Epsilon: 0.1141\n",
      "Episode 434/500, Total Reward: 56.71, Epsilon: 0.1136\n",
      "Episode 435/500, Total Reward: 17.40, Epsilon: 0.1130\n",
      "Episode 436/500, Total Reward: 71.32, Epsilon: 0.1124\n",
      "Episode 437/500, Total Reward: 121.51, Epsilon: 0.1119\n",
      "Episode 438/500, Total Reward: -31.31, Epsilon: 0.1113\n",
      "Episode 439/500, Total Reward: 73.34, Epsilon: 0.1107\n",
      "Episode 440/500, Total Reward: 7.34, Epsilon: 0.1102\n",
      "Episode 441/500, Total Reward: 77.50, Epsilon: 0.1096\n",
      "Episode 442/500, Total Reward: 114.94, Epsilon: 0.1091\n",
      "Episode 443/500, Total Reward: 76.53, Epsilon: 0.1085\n",
      "Episode 444/500, Total Reward: 74.59, Epsilon: 0.1080\n",
      "Episode 445/500, Total Reward: 97.67, Epsilon: 0.1075\n",
      "Episode 446/500, Total Reward: 3.04, Epsilon: 0.1069\n",
      "Episode 447/500, Total Reward: 93.12, Epsilon: 0.1064\n",
      "Episode 448/500, Total Reward: 15.48, Epsilon: 0.1059\n",
      "Episode 449/500, Total Reward: 26.20, Epsilon: 0.1053\n",
      "Episode 450/500, Total Reward: 7.71, Epsilon: 0.1048\n",
      "Episode 451/500, Total Reward: 123.81, Epsilon: 0.1043\n",
      "Episode 452/500, Total Reward: -54.97, Epsilon: 0.1038\n",
      "Episode 453/500, Total Reward: 75.38, Epsilon: 0.1032\n",
      "Episode 454/500, Total Reward: 88.17, Epsilon: 0.1027\n",
      "Episode 455/500, Total Reward: 82.89, Epsilon: 0.1022\n",
      "Episode 456/500, Total Reward: 21.18, Epsilon: 0.1017\n",
      "Episode 457/500, Total Reward: -15.99, Epsilon: 0.1012\n",
      "Episode 458/500, Total Reward: 106.23, Epsilon: 0.1007\n",
      "Episode 459/500, Total Reward: 165.14, Epsilon: 0.1002\n",
      "Episode 460/500, Total Reward: -33.26, Epsilon: 0.0997\n",
      "Episode 461/500, Total Reward: -26.34, Epsilon: 0.0992\n",
      "Episode 462/500, Total Reward: 47.41, Epsilon: 0.0987\n",
      "Episode 463/500, Total Reward: -32.51, Epsilon: 0.0982\n",
      "Episode 464/500, Total Reward: 120.36, Epsilon: 0.0977\n",
      "Episode 465/500, Total Reward: 50.77, Epsilon: 0.0972\n",
      "Episode 466/500, Total Reward: 77.10, Epsilon: 0.0967\n",
      "Episode 467/500, Total Reward: 77.74, Epsilon: 0.0962\n",
      "Episode 468/500, Total Reward: 95.55, Epsilon: 0.0958\n",
      "Episode 469/500, Total Reward: 58.06, Epsilon: 0.0953\n",
      "Episode 470/500, Total Reward: 59.51, Epsilon: 0.0948\n",
      "Episode 471/500, Total Reward: 84.35, Epsilon: 0.0943\n",
      "Episode 472/500, Total Reward: 32.49, Epsilon: 0.0939\n",
      "Episode 473/500, Total Reward: 48.96, Epsilon: 0.0934\n",
      "Episode 474/500, Total Reward: 71.00, Epsilon: 0.0929\n",
      "Episode 475/500, Total Reward: 51.93, Epsilon: 0.0925\n",
      "Episode 476/500, Total Reward: 101.38, Epsilon: 0.0920\n",
      "Episode 477/500, Total Reward: 172.17, Epsilon: 0.0915\n",
      "Episode 478/500, Total Reward: 24.27, Epsilon: 0.0911\n",
      "Episode 479/500, Total Reward: 48.39, Epsilon: 0.0906\n",
      "Episode 480/500, Total Reward: 40.92, Epsilon: 0.0902\n",
      "Episode 481/500, Total Reward: 83.84, Epsilon: 0.0897\n",
      "Episode 482/500, Total Reward: 32.90, Epsilon: 0.0893\n",
      "Episode 483/500, Total Reward: 104.57, Epsilon: 0.0888\n",
      "Episode 484/500, Total Reward: 114.23, Epsilon: 0.0884\n",
      "Episode 485/500, Total Reward: 41.82, Epsilon: 0.0879\n",
      "Episode 486/500, Total Reward: 64.12, Epsilon: 0.0875\n",
      "Episode 487/500, Total Reward: 79.38, Epsilon: 0.0871\n",
      "Episode 488/500, Total Reward: 30.36, Epsilon: 0.0866\n",
      "Episode 489/500, Total Reward: 134.80, Epsilon: 0.0862\n",
      "Episode 490/500, Total Reward: 62.11, Epsilon: 0.0858\n",
      "Episode 491/500, Total Reward: 68.69, Epsilon: 0.0853\n",
      "Episode 492/500, Total Reward: 68.96, Epsilon: 0.0849\n",
      "Episode 493/500, Total Reward: 53.55, Epsilon: 0.0845\n",
      "Episode 494/500, Total Reward: 123.68, Epsilon: 0.0841\n",
      "Episode 495/500, Total Reward: 78.73, Epsilon: 0.0836\n",
      "Episode 496/500, Total Reward: 123.39, Epsilon: 0.0832\n",
      "Episode 497/500, Total Reward: 78.89, Epsilon: 0.0828\n",
      "Episode 498/500, Total Reward: 46.85, Epsilon: 0.0824\n",
      "Episode 499/500, Total Reward: 13.63, Epsilon: 0.0820\n",
      "Episode 500/500, Total Reward: 61.41, Epsilon: 0.0816\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_csv(\"processed_aapl_data.csv\")\n",
    "\n",
    "env = StockTradingEnv(df)\n",
    "state_size = len(env.feature_columns)\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "episodes = 500  # Number of training episodes\n",
    "batch_size = 32  # Batch size for experience replay\n",
    "epsilon_decay = 0.995  # Decay factor for epsilon-greedy exploration\n",
    "min_epsilon = 0.01  # Minimum exploration probability\n",
    "update_target_every = 10  # Frequency to update target network\n",
    "\n",
    "epsilon = 1.0  # Initial exploration probability\n",
    "\n",
    "def train():\n",
    "    global epsilon\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = preprocess_state(df, env.current_step)  # Preprocess initial state\n",
    "        env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.select_action(state.unsqueeze(0), epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = preprocess_state(df, env.current_step)\n",
    "            agent.store_experience(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "        if episode % update_target_every == 0:\n",
    "            agent.update_target_model()\n",
    "            \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "        \n",
    "        print(f\"Episode {episode+1}/{episodes}, Total Reward: {total_reward:.2f}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "    torch.save(agent.model.state_dict(), \"dqn_trading_model.pth\")  # Save trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(-0.005840720376230283),\n",
       " np.float64(-0.005488610934734102),\n",
       " np.float64(-0.006607002604372373),\n",
       " np.float64(-0.002973434066809804),\n",
       " np.float64(0.011458222687551395),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(-0.03208378059294362),\n",
       " np.float64(0.006629467378548215),\n",
       " np.float64(-0.0023397250337158335),\n",
       " np.float64(0.0033874698076296356),\n",
       " np.float64(0.03635738140309934),\n",
       " np.float64(-0.05203811638331526),\n",
       " np.float64(0.015419861070934654),\n",
       " np.float64(-0.0007809470631630277),\n",
       " np.float64(0.0070342817877577036),\n",
       " np.float64(0.010348435556321131),\n",
       " np.float64(-0.005633353034136442),\n",
       " np.float64(-0.012789681234073228),\n",
       " np.float64(-0.020519959712339893),\n",
       " np.float64(-0.06116288767262555),\n",
       " np.float64(-0.024962172242090564),\n",
       " np.float64(0.006012365212122338),\n",
       " np.float64(0.05735497108309362),\n",
       " np.float64(0.029446582693481837),\n",
       " np.float64(0.003276680416062757),\n",
       " np.float64(-0.004678248521615508),\n",
       " np.float64(-0.04469670823026657),\n",
       " np.float64(0.04288892559258126),\n",
       " np.float64(-0.017535994797287898),\n",
       " np.float64(-0.009966531450047524),\n",
       " np.float64(0.027821003101680477),\n",
       " np.float64(-0.019232446618013363),\n",
       " np.float64(0.021970023925753385),\n",
       " np.float64(0.014568707418445887),\n",
       " np.float64(0.009631367506688339),\n",
       " np.float64(0.008412117259912583),\n",
       " np.float64(0.0011180330596601445),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(-0.008545580268219354),\n",
       " np.float64(0.014058579959629884),\n",
       " np.float64(0.011470534627396514),\n",
       " np.float64(-0.00465040137263123),\n",
       " np.float64(-0.008852474025038404),\n",
       " np.float64(0.0011577852445287125),\n",
       " np.float64(-0.004047562148118902),\n",
       " np.float64(-0.03151698649229536),\n",
       " np.float64(-0.005652102866220558),\n",
       " np.float64(-0.0033588785427469303),\n",
       " np.float64(-0.029208476038347926),\n",
       " np.float64(0.01637888576879512),\n",
       " np.float64(-0.004291450888596133),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.012869506763180725),\n",
       " np.float64(-0.005340224912947793),\n",
       " np.float64(-0.01120058407984121),\n",
       " np.float64(0.017974145051774866),\n",
       " np.float64(-0.013058655479507233),\n",
       " np.float64(-0.019194908353070996),\n",
       " np.float64(0.0008549908423349693),\n",
       " np.float64(-0.025059320625400297),\n",
       " np.float64(-0.01956968312868319),\n",
       " np.float64(-0.042204569302861386),\n",
       " np.float64(0.0052877361572562535),\n",
       " np.float64(0.016192241229874744),\n",
       " np.float64(0.014513349466074162),\n",
       " np.float64(-0.025710281296145446),\n",
       " np.float64(0.021870800562345784),\n",
       " np.float64(-0.024015267989008278),\n",
       " np.float64(-0.004838809889825768),\n",
       " np.float64(0.0013448918735032335),\n",
       " np.float64(-0.00506248433861648),\n",
       " np.float64(0.0531671334884142),\n",
       " np.float64(-0.019522734798720933),\n",
       " np.float64(0.0055309272813763785),\n",
       " np.float64(-0.06570656900878682),\n",
       " np.float64(0.007171892336517211),\n",
       " np.float64(0.0345413978796299),\n",
       " np.float64(-0.009348634343030255),\n",
       " np.float64(-0.0202218909267962),\n",
       " np.float64(0.019792496303401734),\n",
       " np.float64(0.002594706839223854),\n",
       " np.float64(-0.026708093911055966),\n",
       " np.float64(0.010529733338775509),\n",
       " np.float64(-0.00021054912126251254),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.001992476339079525),\n",
       " np.float64(0.03711887687756492),\n",
       " np.float64(-0.004154233074759131),\n",
       " np.float64(0.011445071336758419),\n",
       " np.float64(-0.0038071131519277304),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0085174048473693),\n",
       " np.float64(-0.003753525144466125),\n",
       " np.float64(0.005756100521162258),\n",
       " np.float64(-0.028095702300861843),\n",
       " np.float64(-0.014561034134188392),\n",
       " np.float64(0.016840454225829936),\n",
       " np.float64(0.008654826580651705),\n",
       " np.float64(0.012711831873363253),\n",
       " np.float64(0.003033482428410763),\n",
       " np.float64(0.0),\n",
       " np.float64(0.005684818693245983),\n",
       " np.float64(0.004291883883057413),\n",
       " np.float64(0.007713131592022638),\n",
       " np.float64(0.0031030518288253815),\n",
       " np.float64(0.004536964289352831),\n",
       " np.float64(-0.005645611093211999),\n",
       " np.float64(0.019820358361794965),\n",
       " np.float64(-0.0001012464433422189),\n",
       " np.float64(0.010629713147737689),\n",
       " np.float64(0.0004006903214831862),\n",
       " np.float64(0.0009011348294340726),\n",
       " np.float64(-0.005302108684985062),\n",
       " np.float64(-0.007744107821614056),\n",
       " np.float64(-0.013379356124286163),\n",
       " np.float64(-0.006883071647338074),\n",
       " np.float64(0.06496326573133517),\n",
       " np.float64(0.013501694325909063),\n",
       " np.float64(-0.0012459004981847596),\n",
       " np.float64(0.017656693056811765),\n",
       " np.float64(-0.014804334272937026),\n",
       " np.float64(0.012538261069138804),\n",
       " np.float64(0.0007562324450548564),\n",
       " np.float64(0.015207335114606148),\n",
       " np.float64(0.008280604408736484),\n",
       " np.float64(0.0040601162762785225),\n",
       " np.float64(-0.007444146464184892),\n",
       " np.float64(-0.0006481453224464127),\n",
       " np.float64(0.00231631612427598),\n",
       " np.float64(0.01201703686532175),\n",
       " np.float64(-0.0009134645638398591),\n",
       " np.float64(-0.0014627540385116849),\n",
       " np.float64(-0.001281810914518724),\n",
       " np.float64(0.0025669121250159036),\n",
       " np.float64(-0.00777248051734783),\n",
       " np.float64(0.0031333179540792813),\n",
       " np.float64(-0.007533300012118762),\n",
       " np.float64(-0.004258068033606269),\n",
       " np.float64(-0.005856625966396722),\n",
       " np.float64(-0.0011221502136000714),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(-0.026208969650133678),\n",
       " np.float64(-0.02264972958379514),\n",
       " np.float64(0.02239896489390918),\n",
       " np.float64(0.0238049549385279),\n",
       " np.float64(0.03538675129982411),\n",
       " np.float64(0.03399841787499557),\n",
       " np.float64(-0.005624310180802315),\n",
       " np.float64(-0.011660253735130263),\n",
       " np.float64(-8.806247639746473e-05),\n",
       " np.float64(-0.00017607328625647494),\n",
       " np.float64(0.009423158661972998),\n",
       " np.float64(-0.016663790057066103),\n",
       " np.float64(0.001508279392477865),\n",
       " np.float64(0.001860374641941334),\n",
       " np.float64(0.007604568380938244),\n",
       " np.float64(-0.015533099521453693),\n",
       " np.float64(0.007755417580809864),\n",
       " np.float64(-0.0046882476283414565),\n",
       " np.float64(0.004265938244348221),\n",
       " np.float64(0.0004425048828124559),\n",
       " np.float64(0.00743030796298207),\n",
       " np.float64(0.0014926522948140542),\n",
       " np.float64(0.017447006275287443),\n",
       " np.float64(0.0021542438037550375),\n",
       " np.float64(0.008942332406216556),\n",
       " np.float64(-0.0030679477773324798),\n",
       " np.float64(0.005556453050365256),\n",
       " np.float64(-0.0006800493371416871),\n",
       " np.float64(-0.0006805770223540304),\n",
       " np.float64(-0.002979471103124718),\n",
       " np.float64(-0.0005123393663001697),\n",
       " np.float64(-0.003929601008597481),\n",
       " np.float64(0.009005172088324354),\n",
       " np.float64(0.005099859467397569),\n",
       " np.float64(-0.022494745556950448),\n",
       " np.float64(-0.009602846406468496),\n",
       " np.float64(-0.006638732651508508),\n",
       " np.float64(-0.001582837699996531),\n",
       " np.float64(-0.01805533763631893),\n",
       " np.float64(0.0008969277606702977),\n",
       " np.float64(-0.015771973873954955),\n",
       " np.float64(-0.00901398048492174),\n",
       " np.float64(0.014424911586221499),\n",
       " np.float64(0.005887092427544751),\n",
       " np.float64(-0.0016207483264245117),\n",
       " np.float64(-0.027867933030593334),\n",
       " np.float64(0.005937465295598075),\n",
       " np.float64(-0.02508531968133993),\n",
       " np.float64(0.01324379470252543),\n",
       " np.float64(0.026888219932842055),\n",
       " np.float64(-0.00036367775528917687),\n",
       " np.float64(0.0010004603311026992),\n",
       " np.float64(0.015173594724556969),\n",
       " np.float64(0.0006265075872288348),\n",
       " np.float64(-0.005098387113284296),\n",
       " np.float64(0.005034590863013293),\n",
       " np.float64(-0.0019679865721564643),\n",
       " np.float64(-0.000985933590144675),\n",
       " np.float64(-0.00843354072427235),\n",
       " np.float64(-0.009319569404470237),\n",
       " np.float64(0.003744667733207066),\n",
       " np.float64(-0.007188361278969744),\n",
       " np.float64(0.007698619129243508),\n",
       " np.float64(0.009822663583729819),\n",
       " np.float64(0.00981720237115233),\n",
       " np.float64(0.016321745958179867),\n",
       " np.float64(-0.0057042028424064695),\n",
       " np.float64(0.016681371039196077),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0054692007992487),\n",
       " np.float64(0.0012951262845288876),\n",
       " np.float64(0.005777340362963669),\n",
       " np.float64(0.0026577294257193297),\n",
       " np.float64(0.0009405781378537479),\n",
       " np.float64(-0.006577794798611748),\n",
       " np.float64(0.001977777330194778),\n",
       " np.float64(0.006350888383827452),\n",
       " np.float64(-0.004264028576590971),\n",
       " np.float64(-0.0002569268477904252),\n",
       " np.float64(-0.0077957991599365475),\n",
       " np.float64(0.0028492646513918283),\n",
       " np.float64(-0.0011192843831649454),\n",
       " np.float64(0.0050853644574763),\n",
       " np.float64(0.01114829813012107),\n",
       " np.float64(0.009159478993445345),\n",
       " np.float64(0.0010085112088111772),\n",
       " np.float64(0.005373179299545973),\n",
       " np.float64(-0.004175365344467682),\n",
       " np.float64(-0.0017609986119299407),\n",
       " np.float64(0.008064508376086818),\n",
       " np.float64(-8.335113525392711e-05),\n",
       " np.float64(-0.0017501382466152584),\n",
       " np.float64(0.0018367108277276352),\n",
       " np.float64(0.0006666819254557777),\n",
       " np.float64(-0.0009160610315974457),\n",
       " np.float64(0.015920613597403623),\n",
       " np.float64(0.0004923300733547989),\n",
       " np.float64(8.196249496359161e-05),\n",
       " np.float64(-0.002624023803461334),\n",
       " np.float64(-0.0023020536513988876),\n",
       " np.float64(0.06098064786920468),\n",
       " np.float64(-0.001708747345267087),\n",
       " np.float64(0.004279180401318147),\n",
       " np.float64(0.009373965276680232),\n",
       " np.float64(0.009517273444330178),\n",
       " np.float64(0.0038774006809783514),\n",
       " np.float64(0.0028779529092297557),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.002733617292226242),\n",
       " np.float64(0.007220717055148189),\n",
       " np.float64(0.002999295327450535),\n",
       " np.float64(-0.004230193483136007),\n",
       " np.float64(0.0009522074560525778),\n",
       " np.float64(0.001975625687376905),\n",
       " np.float64(0.0004382737208268781),\n",
       " np.float64(0.020439358206379942),\n",
       " np.float64(-0.005937381873727858),\n",
       " np.float64(0.0059009213141312826),\n",
       " np.float64(-0.00314782118506799),\n",
       " np.float64(0.0012918611977986252),\n",
       " np.float64(-0.003727094728620064),\n",
       " np.float64(-0.0023022109656025765),\n",
       " np.float64(0.00331703733892342),\n",
       " np.float64(0.0004312028090910627),\n",
       " np.float64(-0.0015085593367950895),\n",
       " np.float64(0.010576308817941717),\n",
       " np.float64(0.0016374463658370044),\n",
       " np.float64(-0.0049754562235772605),\n",
       " np.float64(0.01050075836145976),\n",
       " np.float64(-0.011452073371192275),\n",
       " np.float64(0.011298640392101881),\n",
       " np.float64(-0.0035355678579678386),\n",
       " np.float64(-0.0019869343097860827),\n",
       " np.float64(0.0017065237073778917),\n",
       " np.float64(0.020726846023140347),\n",
       " np.float64(0.0022252577095879266),\n",
       " np.float64(-0.0013183628076849805),\n",
       " np.float64(-0.0018758356660245668),\n",
       " np.float64(0.0002783884526892185),\n",
       " np.float64(0.0074461193245823985),\n",
       " np.float64(-0.005180631193382307),\n",
       " np.float64(-0.002499656989806083),\n",
       " np.float64(-0.0022275324798918922),\n",
       " np.float64(-0.0011859786053334818),\n",
       " np.float64(-0.01075639663217416),\n",
       " np.float64(0.0012002976988243262),\n",
       " np.float64(-0.00528913951945577),\n",
       " np.float64(0.005529945143004828),\n",
       " np.float64(-0.004441971900719292),\n",
       " np.float64(-0.003682749884559481),\n",
       " np.float64(0.012510732565086417),\n",
       " np.float64(-0.0011934721007551788),\n",
       " np.float64(0.009629542953859985),\n",
       " np.float64(0.006196041446882478),\n",
       " np.float64(-0.00588117422469247),\n",
       " np.float64(0.0007655944874648821),\n",
       " np.float64(-0.0009736379176946265),\n",
       " np.float64(0.02039685387443658),\n",
       " np.float64(0.006344608160485286),\n",
       " np.float64(-0.003050620059655235),\n",
       " np.float64(-0.0036039629273467593),\n",
       " np.float64(0.016583689038518163),\n",
       " np.float64(0.027188423807930222),\n",
       " np.float64(0.006404882174440882),\n",
       " np.float64(-0.004740638744639372),\n",
       " np.float64(0.0045021692949067625),\n",
       " np.float64(0.013965632984041347),\n",
       " np.float64(-0.0025625185114226514),\n",
       " np.float64(-0.00147717233170864),\n",
       " np.float64(-0.03357561703040628),\n",
       " np.float64(0.01524121987442798),\n",
       " np.float64(0.0034089700757067676),\n",
       " np.float64(0.006076100544914115),\n",
       " np.float64(-0.0012338621639616533),\n",
       " np.float64(-0.002990940863065958),\n",
       " np.float64(0.003456363583894993),\n",
       " np.float64(-0.0016897024441830538),\n",
       " np.float64(0.00039058367525129743),\n",
       " np.float64(-0.0059218043401607755),\n",
       " np.float64(0.002749398952920945),\n",
       " np.float64(0.014819195593419222),\n",
       " np.float64(-0.00977809136250429),\n",
       " np.float64(0.0033781868200059586),\n",
       " np.float64(0.005956608527830661),\n",
       " np.float64(-0.0024457078970545774),\n",
       " np.float64(-0.038776756904432066),\n",
       " np.float64(-0.02389580924075483),\n",
       " np.float64(0.008045648354265762),\n",
       " np.float64(-0.009755049536157375),\n",
       " np.float64(-0.005993457936262466),\n",
       " np.float64(-0.01399950868156293),\n",
       " np.float64(0.028607520511739377),\n",
       " np.float64(-0.009088436957342726),\n",
       " np.float64(0.005930629907795993),\n",
       " np.float64(-0.0016452337177512147),\n",
       " np.float64(0.0044633240039194735),\n",
       " np.float64(-0.0031445956994581893),\n",
       " np.float64(-0.014332817800733767),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(-0.005552379737827109),\n",
       " np.float64(-0.0009991076993440633),\n",
       " np.float64(0.018590444414262403),\n",
       " np.float64(0.010511766955265338),\n",
       " np.float64(5.71245224760531e-05),\n",
       " np.float64(-0.005029461488672452),\n",
       " np.float64(-0.005858975444471799),\n",
       " np.float64(-0.02074311807218201),\n",
       " np.float64(0.013984012483612834),\n",
       " np.float64(-0.004655240170756324),\n",
       " np.float64(-0.00730780460507661),\n",
       " np.float64(-0.000942306591482301),\n",
       " np.float64(-0.003713775554581414),\n",
       " np.float64(0.0018342868910647247),\n",
       " np.float64(0.00029522673521396105),\n",
       " np.float64(0.019483988586493927),\n",
       " np.float64(-0.005617659298021484),\n",
       " np.float64(0.0033197864551540324),\n",
       " np.float64(-0.0002902597696504586),\n",
       " np.float64(0.010161421365671271),\n",
       " np.float64(0.014082870213549441),\n",
       " np.float64(-0.010656415952414648),\n",
       " np.float64(-0.0010885022913099047),\n",
       " np.float64(0.0037854223126808618),\n",
       " np.float64(0.0),\n",
       " np.float64(-0.02536990641665175),\n",
       " np.float64(0.0001758737058610668),\n",
       " np.float64(0.002813573917739542),\n",
       " np.float64(-0.010813689991320838),\n",
       " np.float64(0.017904620077962917),\n",
       " np.float64(-0.00017414826572323614),\n",
       " np.float64(0.0046449693526287835),\n",
       " np.float64(0.01138531604115591),\n",
       " np.float64(-0.003714250837053462),\n",
       " np.float64(-0.00011473628770090991),\n",
       " np.float64(-0.00022949890725432382),\n",
       " np.float64(0.005680219928282135),\n",
       " np.float64(0.010326321150154768),\n",
       " np.float64(-0.005082127252220214),\n",
       " np.float64(0.01651628141089978),\n",
       " np.float64(0.0008932908870357139),\n",
       " np.float64(-0.004462723516028273),\n",
       " np.float64(-0.008181142322874006),\n",
       " np.float64(0.0002259507691119875),\n",
       " np.float64(-0.01592855949148166),\n",
       " np.float64(-0.01785099637562163),\n",
       " np.float64(0.002337641838919887),\n",
       " np.float64(-0.020698431034159127),\n",
       " np.float64(-0.005894293007802756),\n",
       " np.float64(0.0027549347290841343),\n",
       " np.float64(0.0020904623951896943),\n",
       " np.float64(-0.04339014681287016),\n",
       " np.float64(-0.02498438945069121),\n",
       " np.float64(0.04179176341340529),\n",
       " np.float64(-0.021407136841659926),\n",
       " np.float64(-0.027516607586757426),\n",
       " np.float64(0.008121236320935114),\n",
       " np.float64(0.04027877312353785),\n",
       " np.float64(0.010017758937775905),\n",
       " np.float64(0.01843737889020672),\n",
       " np.float64(0.03357836254964097),\n",
       " np.float64(-0.0032372553303660692),\n",
       " np.float64(-0.00336360608305638),\n",
       " np.float64(-0.004538834748874213),\n",
       " np.float64(0.00835910805259442),\n",
       " np.float64(0.01739130434782602),\n",
       " np.float64(0.019772086727653196),\n",
       " np.float64(-0.003240776817895029),\n",
       " np.float64(-0.001513561709651571),\n",
       " np.float64(-0.017516254225893157),\n",
       " np.float64(0.006914324079241186),\n",
       " np.float64(0.0034617818915476735),\n",
       " np.float64(-0.0008483720679775302),\n",
       " np.float64(-0.00928284036138462),\n",
       " np.float64(0.010912436013427505),\n",
       " np.float64(0.017180927117594596),\n",
       " np.float64(0.009667771610563676),\n",
       " np.float64(-0.00963020024347565),\n",
       " np.float64(-0.008501410062339178),\n",
       " np.float64(0.0011768182705954565),\n",
       " np.float64(-0.0035263904032848425),\n",
       " np.float64(-0.01527918860478248),\n",
       " np.float64(-0.0003422564606346886),\n",
       " np.float64(-0.022654651313954725),\n",
       " np.float64(-0.014129725629570787),\n",
       " np.float64(-0.023156668763827704),\n",
       " np.float64(0.04747181832882693),\n",
       " np.float64(-0.025641070932568462),\n",
       " np.float64(-0.011049071229740233),\n",
       " np.float64(0.0078087643267686965),\n",
       " np.float64(-0.006556240979370922),\n",
       " np.float64(0.010259219996448046),\n",
       " np.float64(0.019122282988149303),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.018817976423488765),\n",
       " np.float64(-0.004675310583513659),\n",
       " np.float64(0.009858483670689101),\n",
       " np.float64(0.0033880575396722577),\n",
       " np.float64(0.006238262595618485),\n",
       " np.float64(0.01376406590907918),\n",
       " np.float64(-0.0022442164662567197),\n",
       " np.float64(-0.02834004380295302),\n",
       " np.float64(-0.04097223209500786),\n",
       " np.float64(-0.0028964260439499406),\n",
       " np.float64(-0.01391916591199189),\n",
       " np.float64(0.0043573796761997696),\n",
       " np.float64(0.0034830879650340095),\n",
       " np.float64(-0.011569808076732763),\n",
       " np.float64(0.018112290844990175),\n",
       " np.float64(0.023236183736656424),\n",
       " np.float64(0.044175049976818676),\n",
       " np.float64(0.0018122673849250935),\n",
       " np.float64(0.0392334358378226),\n",
       " np.float64(0.007234955218446899),\n",
       " np.float64(0.0048066503134908825),\n",
       " np.float64(0.007041104741230786),\n",
       " np.float64(0.014303974525249882),\n",
       " np.float64(-0.007629956848393562),\n",
       " np.float64(-0.002333116548864569),\n",
       " np.float64(-0.00908844810284146),\n",
       " np.float64(0.00933270870837845),\n",
       " np.float64(-0.00632366473022167),\n",
       " np.float64(-0.003636600431006211),\n",
       " np.float64(0.007085005321862083),\n",
       " np.float64(-0.00250493635597723),\n",
       " np.float64(0.006411609984837478),\n",
       " np.float64(-0.0011149220279607279),\n",
       " np.float64(0.0022854528223204607),\n",
       " np.float64(-0.003605938742006832),\n",
       " np.float64(-0.002128759496952065),\n",
       " np.float64(-0.0033600260416665906),\n",
       " np.float64(0.018033983325483564),\n",
       " np.float64(0.008357844259775139),\n",
       " np.float64(0.0077151421227765445),\n",
       " np.float64(0.0034659261156020833),\n",
       " np.float64(-0.0026806321534424992),\n",
       " np.float64(-0.009097538015844762),\n",
       " np.float64(-0.0024517539289791415),\n",
       " np.float64(0.005490786357877895),\n",
       " np.float64(-0.008217192849414611),\n",
       " np.float64(0.0005244158632197538),\n",
       " np.float64(-0.010272571711309103),\n",
       " np.float64(-0.0005295003530272159),\n",
       " np.float64(-0.016159812244300582),\n",
       " np.float64(0.004362095686057912),\n",
       " np.float64(-0.0055763715074145055),\n",
       " np.float64(-0.0029117250370589236),\n",
       " np.float64(-0.014871295842689638),\n",
       " np.float64(0.012405964371476843),\n",
       " np.float64(-0.0014639105589863868),\n",
       " np.float64(0.007276261464184105),\n",
       " np.float64(-0.0021024225857058368),\n",
       " np.float64(0.01118249721033136),\n",
       " np.float64(-0.01741636197455494),\n",
       " np.float64(0.00804695379661529),\n",
       " np.float64(0.013861960133902085),\n",
       " np.float64(0.013885197602818781),\n",
       " np.float64(-0.001206819841165447),\n",
       " np.float64(-0.012976102660905011),\n",
       " np.float64(0.016765987942405722),\n",
       " np.float64(0.001570449948567677),\n",
       " np.float64(-0.002195150603281596),\n",
       " np.float64(0.0028285227372817174),\n",
       " np.float64(-0.005484476722356303),\n",
       " np.float64(0.0077731671941794365),\n",
       " np.float64(-0.0022931125193318878),\n",
       " np.float64(0.0008879971102034603),\n",
       " np.float64(0.007254315459635515),\n",
       " np.float64(0.009430089762791406),\n",
       " np.float64(-0.0031310983852720652),\n",
       " np.float64(-0.01663153738049642),\n",
       " np.float64(-0.0056026394877303366),\n",
       " np.float64(0.0020008931425198996),\n",
       " np.float64(0.05891012196847921),\n",
       " np.float64(0.029230766201729312),\n",
       " np.float64(0.0028931293952525958),\n",
       " np.float64(0.005192566000918691),\n",
       " np.float64(-0.009374882313117562),\n",
       " np.float64(0.000675966342696437),\n",
       " np.float64(0.00786492102683955),\n",
       " np.float64(-0.006463070049586677),\n",
       " np.float64(0.00645688018971977),\n",
       " np.float64(0.004213170409271933),\n",
       " np.float64(0.0023361406110325375),\n",
       " np.float64(0.014649932223079271),\n",
       " np.float64(0.0199699716884094),\n",
       " np.float64(-0.009743520081563484),\n",
       " np.float64(-0.0019493799992874586),\n",
       " np.float64(4.654839070642091e-05),\n",
       " np.float64(0.002046047129328997),\n",
       " np.float64(0.003109184425569958),\n",
       " np.float64(0.008234635219933112),\n",
       " np.float64(0.00807559184693089),\n",
       " np.float64(0.014929443900127118),\n",
       " np.float64(0.00919366351707507),\n",
       " np.float64(0.011554042206015474),\n",
       " np.float64(0.0032069398228711994),\n",
       " np.float64(-0.006524809463923774),\n",
       " np.float64(-0.016617398046509007),\n",
       " np.float64(-0.00806814434116434),\n",
       " np.float64(-0.013420701218917317),\n",
       " np.float64(0.02528284810226104),\n",
       " np.float64(-0.012419024809011171),\n",
       " np.float64(0.024155227579375122),\n",
       " np.float64(-0.01135112089858977),\n",
       " np.float64(-0.026626123805333735),\n",
       " np.float64(0.0016522884261232888),\n",
       " np.float64(0.0005956269279305844),\n",
       " np.float64(0.007601793741024437),\n",
       " np.float64(-0.010771236332936323),\n",
       " np.float64(0.014380178128097238),\n",
       " np.float64(0.006340908545882743),\n",
       " np.float64(-0.007966174233819099),\n",
       " np.float64(0.020551668709409728),\n",
       " np.float64(0.0035119295649674955),\n",
       " np.float64(0.0067333612859237744),\n",
       " np.float64(0.008888516770601778),\n",
       " np.float64(0.012168564897837396),\n",
       " np.float64(-0.017580909649193458),\n",
       " np.float64(-0.016228835115064504),\n",
       " np.float64(-0.0023183781231314936),\n",
       " np.float64(0.01385346912248357),\n",
       " np.float64(-0.04632606661540715),\n",
       " np.float64(-0.008827896361255657),\n",
       " np.float64(0.03571929946894866),\n",
       " np.float64(-0.02138579977014611),\n",
       " np.float64(0.02203714240284516),\n",
       " np.float64(-0.0043213661105273304),\n",
       " np.float64(-0.023373561697549426),\n",
       " np.float64(0.015230039908633683),\n",
       " np.float64(0.006110055869808782),\n",
       " np.float64(0.009426702418267423),\n",
       " np.float64(-0.0343016187141416),\n",
       " np.float64(0.02189784180603227),\n",
       " np.float64(-0.015923566657894142),\n",
       " np.float64(-0.018770214985259343),\n",
       " np.float64(0.004994334391062192),\n",
       " np.float64(0.026066561083192306),\n",
       " np.float64(0.015352282742306841),\n",
       " np.float64(-0.0663306876617495),\n",
       " np.float64(-0.028388276031117488),\n",
       " np.float64(0.010814068030024424),\n",
       " np.float64(0.03032827475195013),\n",
       " np.float64(-0.006953996076685048),\n",
       " np.float64(-0.019281520296150403),\n",
       " np.float64(-0.050374152639829466),\n",
       " np.float64(-0.009991257453266643),\n",
       " np.float64(-0.02824737448091901),\n",
       " np.float64(0.02467880372075932),\n",
       " np.float64(0.011075675652406693),\n",
       " np.float64(-0.039632089171313604),\n",
       " np.float64(-0.047777923456640434),\n",
       " np.float64(-0.0011300539782478222),\n",
       " np.float64(-0.02539883201814955),\n",
       " np.float64(0.01352372117854284),\n",
       " np.float64(-0.0021760945747847063),\n",
       " np.float64(0.038452690180298794),\n",
       " np.float64(-0.007682101088169085),\n",
       " np.float64(-0.005402401582936724),\n",
       " np.float64(0.03494235317047085),\n",
       " np.float64(-0.04398877048278936),\n",
       " np.float64(-0.011149477579278495),\n",
       " np.float64(-0.03565702657974137),\n",
       " np.float64(0.006587931474645279),\n",
       " np.float64(-0.005719346614357392),\n",
       " np.float64(0.0027871743289679617),\n",
       " np.float64(0.010940217492328702),\n",
       " np.float64(-0.03199766784645952),\n",
       " np.float64(-0.009306220243493389),\n",
       " np.float64(0.012992587843676528),\n",
       " np.float64(-0.031191712567684746),\n",
       " np.float64(-0.025234617278859683),\n",
       " np.float64(-0.03889565792447575),\n",
       " np.float64(-0.025874039720229558),\n",
       " np.float64(0.07042155015286318),\n",
       " np.float64(-0.006489815386804895),\n",
       " np.float64(0.0005123396361303703),\n",
       " np.float64(0.009665299922675603),\n",
       " np.float64(0.0011410718239708434),\n",
       " np.float64(-0.09960737025028885),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.0),\n",
       " np.float64(0.011235381968699469),\n",
       " np.float64(0.004422105110627146),\n",
       " np.float64(0.011116553514043486),\n",
       " np.float64(0.013008215562104792),\n",
       " np.float64(0.010208517113258656),\n",
       " np.float64(-0.0079247178986598),\n",
       " np.float64(0.008738566951587944),\n",
       " np.float64(0.03683031750055597),\n",
       " np.float64(-0.02070835697354591),\n",
       " np.float64(-0.012091062662626379),\n",
       " np.float64(-0.010331737576970035),\n",
       " np.float64(0.008994100299563818),\n",
       " np.float64(0.001326471047810049),\n",
       " np.float64(0.006517569518774092),\n",
       " np.float64(0.006791305952341632),\n",
       " np.float64(0.014536701001068792),\n",
       " np.float64(0.0068549726923362786),\n",
       " np.float64(0.0017404470297813423),\n",
       " np.float64(0.006694248772294809),\n",
       " np.float64(0.0157360715914499),\n",
       " np.float64(-0.0029985311604899963),\n",
       " np.float64(0.005614010612468594),\n",
       " np.float64(-0.00832418607113328),\n",
       " np.float64(-0.00040212029294728075),\n",
       " np.float64(0.0018102309005409813),\n",
       " np.float64(0.00010040793499937234),\n",
       " np.float64(0.019473048345357673),\n",
       " np.float64(0.003593736572596445),\n",
       " np.float64(0.00328656022240434),\n",
       " np.float64(0.014423297148822876),\n",
       " np.float64(-0.0015422791209706499),\n",
       " np.float64(-0.009075134435114825),\n",
       " np.float64(-0.004773946479767227),\n",
       " np.float64(0.001517364434474353),\n",
       " np.float64(-0.01925615771298182),\n",
       " np.float64(0.04908559422631192),\n",
       " np.float64(-0.006507744386150847),\n",
       " np.float64(0.012431298969114279),\n",
       " np.float64(-0.015442759256013837),\n",
       " np.float64(-0.026956999387760156),\n",
       " np.float64(0.00019714722474852955),\n",
       " np.float64(-0.010744173195458246),\n",
       " np.float64(-0.01763655103324462),\n",
       " np.float64(-0.0581194435579555),\n",
       " np.float64(0.015830295186744368),\n",
       " np.float64(0.011979192531362365),\n",
       " np.float64(-0.00439972944660984),\n",
       " np.float64(-0.005681827760158686),\n",
       " ...]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = random.sample(agent.memory, agent.batch_size)\n",
    "states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "states = torch.stack(states)\n",
    "actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "next_states = torch.stack(next_states)\n",
    "dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "q_values = agent.model(states).gather(1, actions)\n",
    "next_q_values = agent.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "target_q_values = rewards + (agent.gamma * next_q_values * (1 - dones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_q_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoreq312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
