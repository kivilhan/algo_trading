{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "x shape: (2427, 20, 13)\n",
      "y shape: (2427, 1)     \n",
      "close shape: (2427, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "import data_pipes\n",
    "\n",
    "df_path = \"processed_aapl_data.csv\"\n",
    "data = data_pipes.process_df_3(df_path)\n",
    "x, y, close = data[\"x\"], data[\"y\"], data[\"close\"]\n",
    "# y = (y - y.min()) / (y.max() - y.min())\n",
    "# y = y - y.mean()\n",
    "print(f\"\"\"\n",
    "x shape: {x.shape}\n",
    "y shape: {y.shape}     \n",
    "close shape: {close.shape} \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@variational_estimator\n",
    "class BayesianGeorgia(nn.Module):\n",
    "    def __init__(self, config, win_past=20, features=13, bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Flatten())\n",
    "\n",
    "        # Construct Bayesian layers with activation and dropout\n",
    "        for idx in range(len(config['neurons'])):\n",
    "            # Choose activation function\n",
    "            activation_fn = {\n",
    "                'relu': nn.ReLU(),\n",
    "                'selu': nn.SELU(),\n",
    "                'sigmoid': nn.Sigmoid(),\n",
    "                'tanh': nn.Tanh(),\n",
    "                'none': None\n",
    "            }.get(config['activations'][idx].lower(), None)\n",
    "\n",
    "            # Define layer input and output dimensions\n",
    "            in_features = features * win_past if idx == 0 else config['neurons'][idx - 1]\n",
    "            out_features = config['neurons'][idx]\n",
    "\n",
    "            # Replace nn.Linear with BayesianLinear\n",
    "            self.layers.append(BayesianLinear(in_features, out_features, bias=bias))\n",
    "\n",
    "            # Add activation if specified\n",
    "            if activation_fn is not None:\n",
    "                self.layers.append(activation_fn)\n",
    "\n",
    "            # Add dropout\n",
    "            dropout_rate = config['dropouts'][idx]\n",
    "            self.layers.append(nn.Dropout(dropout_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 34.5468\n",
      "Epoch: 1, Loss: 34.5098\n",
      "Epoch: 2, Loss: 34.4961\n",
      "Epoch: 3, Loss: 34.4917\n",
      "Epoch: 4, Loss: 34.5241\n",
      "Epoch: 5, Loss: 34.4611\n",
      "Epoch: 6, Loss: 34.4817\n",
      "Epoch: 7, Loss: 34.4912\n",
      "Epoch: 8, Loss: 34.5426\n",
      "Epoch: 9, Loss: 34.4577\n",
      "Epoch: 10, Loss: 34.4689\n",
      "Epoch: 11, Loss: 34.4751\n",
      "Epoch: 12, Loss: 34.4355\n",
      "Epoch: 13, Loss: 34.4016\n",
      "Epoch: 14, Loss: 34.4762\n",
      "Epoch: 15, Loss: 34.4091\n",
      "Epoch: 16, Loss: 34.4178\n",
      "Epoch: 17, Loss: 34.3784\n",
      "Epoch: 18, Loss: 34.3425\n",
      "Epoch: 19, Loss: 34.3924\n",
      "Epoch: 20, Loss: 34.4186\n",
      "Epoch: 21, Loss: 34.4576\n",
      "Epoch: 22, Loss: 34.3796\n",
      "Epoch: 23, Loss: 34.3527\n",
      "Epoch: 24, Loss: 34.3335\n",
      "Epoch: 25, Loss: 34.2983\n",
      "Epoch: 26, Loss: 34.4076\n",
      "Epoch: 27, Loss: 34.3918\n",
      "Epoch: 28, Loss: 34.3081\n",
      "Epoch: 29, Loss: 34.3434\n",
      "Epoch: 30, Loss: 34.2937\n",
      "Epoch: 31, Loss: 34.3527\n",
      "Epoch: 32, Loss: 34.3661\n",
      "Epoch: 33, Loss: 34.3324\n",
      "Epoch: 34, Loss: 34.3239\n",
      "Epoch: 35, Loss: 34.2665\n",
      "Epoch: 36, Loss: 34.2909\n",
      "Epoch: 37, Loss: 34.3451\n",
      "Epoch: 38, Loss: 34.2997\n",
      "Epoch: 39, Loss: 34.3177\n",
      "Epoch: 40, Loss: 34.2732\n",
      "Epoch: 41, Loss: 34.3210\n",
      "Epoch: 42, Loss: 34.3062\n",
      "Epoch: 43, Loss: 34.2335\n",
      "Epoch: 44, Loss: 34.2987\n",
      "Epoch: 45, Loss: 34.2628\n",
      "Epoch: 46, Loss: 34.2103\n",
      "Epoch: 47, Loss: 34.2312\n",
      "Epoch: 48, Loss: 34.2942\n",
      "Epoch: 49, Loss: 34.1705\n",
      "Epoch: 50, Loss: 34.1954\n",
      "Epoch: 51, Loss: 34.2661\n",
      "Epoch: 52, Loss: 34.2626\n",
      "Epoch: 53, Loss: 34.1785\n",
      "Epoch: 54, Loss: 34.2271\n",
      "Epoch: 55, Loss: 34.2226\n",
      "Epoch: 56, Loss: 34.2298\n",
      "Epoch: 57, Loss: 34.3278\n",
      "Epoch: 58, Loss: 34.2794\n",
      "Epoch: 59, Loss: 34.2195\n",
      "Epoch: 60, Loss: 34.2324\n",
      "Epoch: 61, Loss: 34.2121\n",
      "Epoch: 62, Loss: 34.2153\n",
      "Epoch: 63, Loss: 34.2439\n",
      "Epoch: 64, Loss: 34.1090\n",
      "Epoch: 65, Loss: 34.1937\n",
      "Epoch: 66, Loss: 34.2100\n",
      "Epoch: 67, Loss: 34.2197\n",
      "Epoch: 68, Loss: 34.1506\n",
      "Epoch: 69, Loss: 34.1467\n",
      "Epoch: 70, Loss: 34.1014\n",
      "Epoch: 71, Loss: 34.1673\n",
      "Epoch: 72, Loss: 34.2057\n",
      "Epoch: 73, Loss: 34.1683\n",
      "Epoch: 74, Loss: 34.1063\n",
      "Epoch: 75, Loss: 34.1298\n",
      "Epoch: 76, Loss: 34.1733\n",
      "Epoch: 77, Loss: 34.1459\n",
      "Epoch: 78, Loss: 34.1208\n",
      "Epoch: 79, Loss: 34.1121\n",
      "Epoch: 80, Loss: 34.1872\n",
      "Epoch: 81, Loss: 34.1776\n",
      "Epoch: 82, Loss: 34.1203\n",
      "Epoch: 83, Loss: 34.1500\n",
      "Epoch: 84, Loss: 34.1357\n",
      "Epoch: 85, Loss: 34.1614\n",
      "Epoch: 86, Loss: 34.1319\n",
      "Epoch: 87, Loss: 34.1630\n",
      "Epoch: 88, Loss: 34.0869\n",
      "Epoch: 89, Loss: 34.0952\n",
      "Epoch: 90, Loss: 34.0801\n",
      "Epoch: 91, Loss: 34.0980\n",
      "Epoch: 92, Loss: 34.0693\n",
      "Epoch: 93, Loss: 34.0831\n",
      "Epoch: 94, Loss: 34.1030\n",
      "Epoch: 95, Loss: 34.0753\n",
      "Epoch: 96, Loss: 34.0704\n",
      "Epoch: 97, Loss: 34.1103\n",
      "Epoch: 98, Loss: 34.0899\n",
      "Epoch: 99, Loss: 34.0805\n",
      "Epoch: 100, Loss: 34.0213\n",
      "Epoch: 101, Loss: 34.0821\n",
      "Epoch: 102, Loss: 34.0640\n",
      "Epoch: 103, Loss: 34.0375\n",
      "Epoch: 104, Loss: 34.0105\n",
      "Epoch: 105, Loss: 34.0264\n",
      "Epoch: 106, Loss: 34.0381\n",
      "Epoch: 107, Loss: 33.9673\n",
      "Epoch: 108, Loss: 34.0117\n",
      "Epoch: 109, Loss: 33.9983\n",
      "Epoch: 110, Loss: 33.9664\n",
      "Epoch: 111, Loss: 34.0557\n",
      "Epoch: 112, Loss: 34.0110\n",
      "Epoch: 113, Loss: 34.0094\n",
      "Epoch: 114, Loss: 33.9825\n",
      "Epoch: 115, Loss: 33.9988\n",
      "Epoch: 116, Loss: 33.9898\n",
      "Epoch: 117, Loss: 33.9688\n",
      "Epoch: 118, Loss: 33.9890\n",
      "Epoch: 119, Loss: 33.9714\n",
      "Epoch: 120, Loss: 33.9357\n",
      "Epoch: 121, Loss: 33.9808\n",
      "Epoch: 122, Loss: 34.0038\n",
      "Epoch: 123, Loss: 34.0195\n",
      "Epoch: 124, Loss: 33.9888\n",
      "Epoch: 125, Loss: 33.9567\n",
      "Epoch: 126, Loss: 33.9945\n",
      "Epoch: 127, Loss: 33.9331\n",
      "Epoch: 128, Loss: 33.9556\n",
      "Epoch: 129, Loss: 33.9436\n",
      "Epoch: 130, Loss: 34.0505\n",
      "Epoch: 131, Loss: 33.9637\n",
      "Epoch: 132, Loss: 33.8784\n",
      "Epoch: 133, Loss: 33.9403\n",
      "Epoch: 134, Loss: 33.9356\n",
      "Epoch: 135, Loss: 33.9726\n",
      "Epoch: 136, Loss: 33.9586\n",
      "Epoch: 137, Loss: 33.9410\n",
      "Epoch: 138, Loss: 33.9046\n",
      "Epoch: 139, Loss: 33.8889\n",
      "Epoch: 140, Loss: 33.9687\n",
      "Epoch: 141, Loss: 33.9213\n",
      "Epoch: 142, Loss: 33.8993\n",
      "Epoch: 143, Loss: 33.9654\n",
      "Epoch: 144, Loss: 33.8941\n",
      "Epoch: 145, Loss: 33.9655\n",
      "Epoch: 146, Loss: 33.8965\n",
      "Epoch: 147, Loss: 33.9076\n",
      "Epoch: 148, Loss: 33.9044\n",
      "Epoch: 149, Loss: 33.8677\n",
      "Epoch: 150, Loss: 33.8426\n",
      "Epoch: 151, Loss: 33.8340\n",
      "Epoch: 152, Loss: 33.9380\n",
      "Epoch: 153, Loss: 33.9112\n",
      "Epoch: 154, Loss: 33.8485\n",
      "Epoch: 155, Loss: 33.8615\n",
      "Epoch: 156, Loss: 33.8489\n",
      "Epoch: 157, Loss: 33.8399\n",
      "Epoch: 158, Loss: 33.9226\n",
      "Epoch: 159, Loss: 33.9157\n",
      "Epoch: 160, Loss: 33.8951\n",
      "Epoch: 161, Loss: 33.7870\n",
      "Epoch: 162, Loss: 33.7953\n",
      "Epoch: 163, Loss: 33.8546\n",
      "Epoch: 164, Loss: 33.8539\n",
      "Epoch: 165, Loss: 33.8312\n",
      "Epoch: 166, Loss: 33.8573\n",
      "Epoch: 167, Loss: 33.8381\n",
      "Epoch: 168, Loss: 33.8038\n",
      "Epoch: 169, Loss: 33.8520\n",
      "Epoch: 170, Loss: 33.7323\n",
      "Epoch: 171, Loss: 33.8362\n",
      "Epoch: 172, Loss: 33.9103\n",
      "Epoch: 173, Loss: 33.7983\n",
      "Epoch: 174, Loss: 33.8459\n",
      "Epoch: 175, Loss: 33.8899\n",
      "Epoch: 176, Loss: 33.8227\n",
      "Epoch: 177, Loss: 33.7626\n",
      "Epoch: 178, Loss: 33.8797\n",
      "Epoch: 179, Loss: 33.8114\n",
      "Epoch: 180, Loss: 33.7893\n",
      "Epoch: 181, Loss: 33.7501\n",
      "Epoch: 182, Loss: 33.7991\n",
      "Epoch: 183, Loss: 33.8126\n",
      "Epoch: 184, Loss: 33.8490\n",
      "Epoch: 185, Loss: 33.7734\n",
      "Epoch: 186, Loss: 33.7377\n",
      "Epoch: 187, Loss: 33.7976\n",
      "Epoch: 188, Loss: 33.7393\n",
      "Epoch: 189, Loss: 33.7481\n",
      "Epoch: 190, Loss: 33.7392\n",
      "Epoch: 191, Loss: 33.7659\n",
      "Epoch: 192, Loss: 33.7677\n",
      "Epoch: 193, Loss: 33.7946\n",
      "Epoch: 194, Loss: 33.7263\n",
      "Epoch: 195, Loss: 33.7068\n",
      "Epoch: 196, Loss: 33.7862\n",
      "Epoch: 197, Loss: 33.6909\n",
      "Epoch: 198, Loss: 33.7457\n",
      "Epoch: 199, Loss: 33.7185\n",
      "Epoch: 200, Loss: 33.7229\n",
      "Epoch: 201, Loss: 33.7343\n",
      "Epoch: 202, Loss: 33.7189\n",
      "Epoch: 203, Loss: 33.8055\n",
      "Epoch: 204, Loss: 33.6942\n",
      "Epoch: 205, Loss: 33.7378\n",
      "Epoch: 206, Loss: 33.7471\n",
      "Epoch: 207, Loss: 33.7128\n",
      "Epoch: 208, Loss: 33.7285\n",
      "Epoch: 209, Loss: 33.7184\n",
      "Epoch: 210, Loss: 33.6563\n",
      "Epoch: 211, Loss: 33.7176\n",
      "Epoch: 212, Loss: 33.7187\n",
      "Epoch: 213, Loss: 33.6892\n",
      "Epoch: 214, Loss: 33.7053\n",
      "Epoch: 215, Loss: 33.6895\n",
      "Epoch: 216, Loss: 33.6830\n",
      "Epoch: 217, Loss: 33.6672\n",
      "Epoch: 218, Loss: 33.6502\n",
      "Epoch: 219, Loss: 33.6640\n",
      "Epoch: 220, Loss: 33.6939\n",
      "Epoch: 221, Loss: 33.7199\n",
      "Epoch: 222, Loss: 33.6836\n",
      "Epoch: 223, Loss: 33.6971\n",
      "Epoch: 224, Loss: 33.6498\n",
      "Epoch: 225, Loss: 33.6343\n",
      "Epoch: 226, Loss: 33.6852\n",
      "Epoch: 227, Loss: 33.6418\n",
      "Epoch: 228, Loss: 33.6889\n",
      "Epoch: 229, Loss: 33.6485\n",
      "Epoch: 230, Loss: 33.6537\n",
      "Epoch: 231, Loss: 33.6688\n",
      "Epoch: 232, Loss: 33.5905\n",
      "Epoch: 233, Loss: 33.6774\n",
      "Epoch: 234, Loss: 33.6788\n",
      "Epoch: 235, Loss: 33.6232\n",
      "Epoch: 236, Loss: 33.6119\n",
      "Epoch: 237, Loss: 33.6234\n",
      "Epoch: 238, Loss: 33.6818\n",
      "Epoch: 239, Loss: 33.6387\n",
      "Epoch: 240, Loss: 33.6064\n",
      "Epoch: 241, Loss: 33.5769\n",
      "Epoch: 242, Loss: 33.5831\n",
      "Epoch: 243, Loss: 33.5970\n",
      "Epoch: 244, Loss: 33.6303\n",
      "Epoch: 245, Loss: 33.6277\n",
      "Epoch: 246, Loss: 33.6293\n",
      "Epoch: 247, Loss: 33.6437\n",
      "Epoch: 248, Loss: 33.5653\n",
      "Epoch: 249, Loss: 33.6390\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'neurons': [128, 256, 256, 128, 32, 1],\n",
    "    'activations': ['none', 'selu', 'selu', 'selu', 'selu', 'tanh'],\n",
    "    'dropouts': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "}\n",
    "\n",
    "inputs = torch.tensor(x, dtype=torch.float32)\n",
    "targets = torch.tensor(y, dtype=torch.float32)\n",
    "model = BayesianGeorgia(config)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "criterion = nn.MSELoss()  # or your preferred loss function\n",
    "kl_weight = 0.1           # regularization strength\n",
    "\n",
    "epochs = 250\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "    data_loss = criterion(outputs, targets)\n",
    "\n",
    "    # Blitz computes complexity (KL-divergence)\n",
    "    complexity_loss = model.nn_kl_divergence() / len(inputs)\n",
    "\n",
    "    loss = data_loss + kl_weight * complexity_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}, Loss: {loss.item():.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoreq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
