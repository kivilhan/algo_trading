{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import numpy as np\n",
    "import data_pipes\n",
    "\n",
    "df_path = \"processed_aapl_data.csv\"\n",
    "data = data_pipes.process_df_3(df_path)\n",
    "x, y, close = data[\"x\"], data[\"y\"], data[\"close\"]\n",
    "# y = (y - y.min()) / (y.max() - y.min())\n",
    "# y = y - y.mean()\n",
    "print(f\"\"\"\n",
    "x shape: {x.shape}\n",
    "y shape: {y.shape}     \n",
    "close shape: {close.shape} \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class L1LossWithStdReward(nn.Module):\n",
    "    def __init__(self, reward_weight=1.0, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Initializes the custom loss.\n",
    "\n",
    "        Args:\n",
    "            reward_weight (float): Hyperparameter controlling the weight of the standard deviation reward.\n",
    "            reduction (str): Specifies the reduction to apply to the L1 loss ('mean' or 'sum').\n",
    "        \"\"\"\n",
    "        super(L1LossWithStdReward, self).__init__()\n",
    "        self.l1_loss = nn.L1Loss(reduction=reduction)\n",
    "        self.reward_weight = reward_weight\n",
    "        \n",
    "    def forward(self, outputs, targets):\n",
    "        # Compute the standard L1 loss between outputs and targets.\n",
    "        loss = self.l1_loss(outputs, targets)\n",
    "        \n",
    "        # Calculate the standard deviation of the outputs.\n",
    "        # This computes the std over all elements in the output tensor.\n",
    "        std_reward = torch.std(outputs)\n",
    "        \n",
    "        # Subtract the reward term (i.e. reward for high std) from the L1 loss.\n",
    "        total_loss = loss - self.reward_weight * std_reward\n",
    "        return total_loss\n",
    "        \n",
    "class georgia_1(nn.Module):\n",
    "    def __init__(self, config, win_past=20, features=13, bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(nn.Flatten())\n",
    "\n",
    "        for idx in range(len(config['neurons'])):\n",
    "            if config['activations'][idx] == 'relu':\n",
    "                activation = nn.ReLU()\n",
    "            elif config['activations'][idx] == 'selu':\n",
    "                activation = nn.SELU()\n",
    "            elif config['activations'][idx] == 'sigmoid':\n",
    "                activation = nn.Sigmoid()\n",
    "            elif config['activations'][idx] == 'tanh':\n",
    "                activation = nn.Tanh()\n",
    "            elif config['activations'][idx] == 'none':\n",
    "                activation = \"none\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unrecognized activation function at index {idx}: {config['activations'][idx]}\")\n",
    "\n",
    "            if idx == 0:\n",
    "                self.layers.append(nn.Linear(features*win_past, config['neurons'][idx], bias=bias))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(config['neurons'][idx - 1], config['neurons'][idx], bias=bias))\n",
    "\n",
    "            if activation != 'none':\n",
    "                self.layers.append(activation)\n",
    "\n",
    "            self.layers.append(nn.Dropout(config['dropouts'][idx]))\n",
    "        \n",
    "        # self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                \n",
    "                nn.init.trunc_normal_(m.weight, mean=0, std=1)  # Set mean=0, std=1\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)  # Optional: Initialize biases to 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "v_split = 0.5\n",
    "v_cutoff = int(len(x)*(1 - v_split))\n",
    "x_t = torch.tensor(x[:v_cutoff], dtype=torch.float32).to(device)\n",
    "x_v = torch.tensor(x[v_cutoff:], dtype=torch.float32).to(device)\n",
    "y_t = torch.tensor(y[:v_cutoff], dtype=torch.float32).to(device)\n",
    "y_v = torch.tensor(y[v_cutoff:], dtype=torch.float32).to(device)\n",
    "close_t = close[:v_cutoff]\n",
    "close_v = close[v_cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from torch import optim\n",
    "import torch_ops as ops\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    data,\n",
    "    v_split,\n",
    "    optimizer,\n",
    "    loss_fn,\n",
    "    batch_size=32,\n",
    "    epochs=250,\n",
    "    verbose=True\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    x, y, close = data[\"x\"], data[\"y\"], data[\"close\"]\n",
    "    v_cutoff = int(len(x)*(1 - v_split))\n",
    "    x_t = torch.tensor(x[:v_cutoff], dtype=torch.float32).to(device)\n",
    "    y_t = torch.tensor(y[:v_cutoff], dtype=torch.float32).to(device)\n",
    "    close_t = torch.tensor(close[:v_cutoff], dtype=torch.float32).to(device)\n",
    "    x_v = torch.tensor(x[v_cutoff:], dtype=torch.float32).to(device)\n",
    "    y_v = torch.tensor(y[v_cutoff:], dtype=torch.float32).to(device)\n",
    "    close_v = torch.tensor(close[v_cutoff:], dtype=torch.float32).to(device)\n",
    "    \n",
    "    used_data = {\n",
    "        \"x_t\": x_t,\n",
    "        \"y_t\": y_t,\n",
    "        \"close_t\": close_t,\n",
    "        \"x_v\": x_v,\n",
    "        \"y_v\": y_v,\n",
    "        \"close_v\": close_v,\n",
    "    }\n",
    "\n",
    "    loss = ops.test_loop(x_t, y_t, model, loss_fn, batch_size)\n",
    "    test_loss = ops.test_loop(x_v, y_v, model, loss_fn, batch_size)\n",
    "    best_test_loss = test_loss\n",
    "    if verbose:\n",
    "        print(\"---------- Epoch 0 ----------\")\n",
    "        print(f\"loss: {loss:1.4f}, test loss: {test_loss:1.4f}\")\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    for epoch in range(epochs):\n",
    "        loss = ops.train_loop(x_t, y_t, model, loss_fn, optimizer, batch_size)\n",
    "        # print(f\"epoch: {epoch}, loss: {loss:1.4f}\")\n",
    "        test_loss_new = ops.test_loop(x_v, y_v, model, loss_fn, batch_size)\n",
    "        best_test_loss = min(best_test_loss, test_loss_new)\n",
    "\n",
    "        if verbose:\n",
    "            if test_loss_new < test_loss or epoch == epochs - 1:\n",
    "                print(f\"---------- Epoch {epoch + 1} ----------\")\n",
    "                print(f\"loss: {loss:1.4f}, test loss: {test_loss_new:1.4f}\")\n",
    "            \n",
    "            # improvement = test_loss_new < test_loss\n",
    "            # if improvement:\n",
    "            #     test_loss = test_loss_new\n",
    "            #     print(f\"Checkpoint: test loss = {test_loss:1.4f} <---------------\")\n",
    "            #     torch.save(model.state_dict(), r\"models/hanzo_0\")\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    print(f\"Training done in {t1 - t0:5.2f} s.\")\n",
    "\n",
    "    return {\"loss\": loss, \"test_loss\": test_loss, \"used_data\": used_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_past, features = x.shape[1], x.shape[2]\n",
    "verbose = True\n",
    "# model_config = random_model_config()\n",
    "model_config = {\n",
    "    'neurons': [128, 256, 256, 128, 32, 1],\n",
    "    'activations': ['none', 'selu', 'selu', 'selu', 'selu', 'tanh'],\n",
    "    'dropouts': [0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "}\n",
    "\n",
    "model0 = georgia_1(\n",
    "    model_config,\n",
    "    win_past=win_past,\n",
    "    features=features,\n",
    "    bias=False\n",
    "    ).to(device)\n",
    "\n",
    "model1 = georgia_1(\n",
    "    model_config,\n",
    "    win_past=win_past,\n",
    "    features=features,\n",
    "    bias=False\n",
    "    ).to(device)\n",
    "\n",
    "lr = 1e-7\n",
    "l2_decay = 0\n",
    "optimizer = optim.NAdam(\n",
    "    params=model0.parameters(),\n",
    "    lr=lr,\n",
    "    # weight_decay=l2_decay\n",
    "    )\n",
    "\n",
    "# loss_fn = L1LossWithStdReward(reward_weight=0.3)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "epochs = 500\n",
    "batch_size = 32\n",
    "results0 = train_model(\n",
    "    model0,\n",
    "    data,\n",
    "    v_split=0.5,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=False\n",
    ")\n",
    "results1 = train_model(\n",
    "    model1,\n",
    "    data,\n",
    "    v_split=0.5,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_hist = results0['used_data']['close_v']\n",
    "mod_out = model0(results0['used_data']['x_v'])\n",
    "\n",
    "close_hist = close_hist.to('cpu').detach().numpy()\n",
    "mod_out = mod_out.to('cpu').detach().numpy()\n",
    "\n",
    "score_thr = 0.1\n",
    "ctr_thr = 3\n",
    "\n",
    "stack_list = [mod_out[i:len(mod_out) - ctr_thr + i + 1] for i in range(ctr_thr)]\n",
    "mod_hist_stack = np.stack(stack_list)\n",
    "\n",
    "thr_exceed_pos = np.greater(mod_hist_stack, score_thr).sum(axis=0)\n",
    "thr_exceed_neg = np.less(mod_hist_stack, -score_thr).sum(axis=0)\n",
    "thr_exceed_not = np.logical_and(\n",
    "    np.greater(mod_hist_stack, -score_thr),\n",
    "    np.less(mod_hist_stack, score_thr)\n",
    ").sum(axis=0)\n",
    "\n",
    "longs = np.greater_equal(thr_exceed_pos, ctr_thr)\n",
    "shorts = np.greater_equal(thr_exceed_neg, ctr_thr)\n",
    "closes = np.greater_equal(thr_exceed_not, ctr_thr)\n",
    "\n",
    "close_trim = close_hist[ctr_thr:]\n",
    "\n",
    "base_gains = np.ones((len(close_trim), ))\n",
    "mod_gains = np.ones((len(close_trim), ))\n",
    "\n",
    "handicap = 0\n",
    "pos = 0\n",
    "pos_log = np.zeros((0, close_trim.shape[1]), np.int32)\n",
    "\n",
    "for t in range(1, len(close_trim)):\n",
    "    if longs[t]:\n",
    "        if pos != 1:\n",
    "            mod_gains[t] -= handicap\n",
    "        pos = 1\n",
    "\n",
    "    elif shorts[t]:\n",
    "        if pos != -1:\n",
    "            mod_gains[t] -= handicap\n",
    "        pos = -1\n",
    "\n",
    "    elif thr_exceed_not[t]:\n",
    "        pos = 0\n",
    "\n",
    "    if pos == 1:\n",
    "        mod_gains[t] = mod_gains[t - 1] * (close_trim[t] / close_trim[t - 1])\n",
    "\n",
    "    elif pos == -1:\n",
    "        mod_gains[t] = mod_gains[t - 1] * (close_trim[t - 1] / close_trim[t])\n",
    "\n",
    "    else:\n",
    "        mod_gains[t] = mod_gains[t - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8k/fwvhflfs2xl4y4m2g9nlgfcw0000gn/T/ipykernel_56960/3697136071.py:2: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  mod_gains[t] = mod_gains[t - 1] * (close_trim[t] / close_trim[t - 1])\n"
     ]
    }
   ],
   "source": [
    "t = 10\n",
    "mod_gains[t] = mod_gains[t - 1] * (close_trim[t] / close_trim[t - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mod_gains[t - 1] * (close_trim[t] / close_trim[t - 1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1211,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_gains.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "\n",
    "if pos == 1:\n",
    "    mod_gains[t] = mod_gains[t - 1] * close_trim[t] / close_trim[t - 1]\n",
    "elif pos == -1:\n",
    "    mod_gains[t] = mod_gains[t - 1] / close_trim[t] * close_trim[t - 1]\n",
    "else:\n",
    "    mod_gains[t] = mod_gains[t-1]\n",
    "\n",
    "base_gains[t] = close_trim[t] / close_trim[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_trim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(close_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# pred = model(x_v).to('cpu').detach().numpy()\n",
    "# label = y_v.to('cpu').detach().numpy()\n",
    "\n",
    "pred = model1(x_v).to('cpu').detach().numpy()\n",
    "label = y_v.to('cpu').detach().numpy()\n",
    "sym_index = np.arange(pred.shape[1])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.title(\"model prediction vs labels\")\n",
    "plt.xlabel(\"symbols\")\n",
    "plt.ylabel(\"1% => 0.5\")\n",
    "\n",
    "plt.plot(pred, label=\"pred\")\n",
    "plt.plot(label, label=\"label\")\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoreq312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
