{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"processed_aapl_data.csv\")\n",
    "current_step = 100\n",
    "window_size = 20\n",
    "\n",
    "df_window = df[current_step - window_size:current_step].copy()\n",
    "norm_cols = [\n",
    "    'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "    'SMA_10', 'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD',\n",
    "    'MACD_Signal', 'MACD_Hist'\n",
    "]\n",
    "df_window[norm_cols] = (df_window[norm_cols] - df_window[norm_cols].min()) / (df_window[norm_cols].max() - df_window[norm_cols].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Open            1.332499e+00\n",
       "High            1.529999e+00\n",
       "Low             2.314999e+00\n",
       "Close           2.129999e+00\n",
       "Adj Close       1.910696e+00\n",
       "Volume          2.055360e+08\n",
       "Doji            1.000000e+02\n",
       "Engulfing       1.000000e+02\n",
       "Hammer          0.000000e+00\n",
       "Morning Star    0.000000e+00\n",
       "Evening Star    0.000000e+00\n",
       "SMA_10          1.000500e+00\n",
       "SMA_50          1.685499e-01\n",
       "EMA_10          1.082490e+00\n",
       "EMA_50          3.435030e-01\n",
       "RSI_14          2.153329e+01\n",
       "MACD            3.592072e-01\n",
       "MACD_Signal     3.066424e-01\n",
       "MACD_Hist       1.150150e-01\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
       "       'Engulfing', 'Hammer', 'Morning Star', 'Evening Star', 'SMA_10',\n",
       "       'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal',\n",
       "       'MACD_Hist'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed! Processed data saved as 'processed_aapl_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import talib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "aapl_df = pd.read_csv(\"aapl_data.csv\")\n",
    "\n",
    "# Ensure the Date column is in datetime format\n",
    "if 'Date' in aapl_df.columns:\n",
    "    aapl_df['Date'] = pd.to_datetime(aapl_df['Date'])\n",
    "\n",
    "# Sort by date\n",
    "aapl_df = aapl_df.sort_values(by='Date', ascending=True) if 'Date' in aapl_df.columns else aapl_df\n",
    "\n",
    "### 1. Candlestick Pattern Recognition ###\n",
    "patterns = {\n",
    "    \"Doji\": talib.CDLDOJI,\n",
    "    \"Engulfing\": talib.CDLENGULFING,\n",
    "    \"Hammer\": talib.CDLHAMMER,\n",
    "    \"Morning Star\": talib.CDLMORNINGSTAR,\n",
    "    \"Evening Star\": talib.CDLEVENINGSTAR,\n",
    "}\n",
    "\n",
    "for pattern_name, pattern_func in patterns.items():\n",
    "    aapl_df[pattern_name] = pattern_func(aapl_df['Open'], aapl_df['High'], aapl_df['Low'], aapl_df['Close'])\n",
    "\n",
    "### 2. Technical Indicators ###\n",
    "aapl_df['SMA_10'] = talib.SMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['SMA_50'] = talib.SMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['EMA_10'] = talib.EMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['EMA_50'] = talib.EMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['RSI_14'] = talib.RSI(aapl_df['Close'], timeperiod=14)\n",
    "aapl_df['MACD'], aapl_df['MACD_Signal'], aapl_df['MACD_Hist'] = talib.MACD(aapl_df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "# ### 3. Normalization ###\n",
    "# price_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'SMA_10', 'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "# aapl_df[price_columns] = (aapl_df[price_columns] - aapl_df[price_columns].min()) / (aapl_df[price_columns].max() - aapl_df[price_columns].min())\n",
    "\n",
    "# ### 4. Windowed Representation ###\n",
    "# window_size = 3  \n",
    "# feature_columns = ['Close', 'Volume', 'SMA_10', 'SMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "\n",
    "# for col in feature_columns:\n",
    "#     for i in range(1, window_size + 1):\n",
    "#         aapl_df[f\"{col}_lag{i}\"] = aapl_df[col].shift(i)\n",
    "\n",
    "# aapl_df = aapl_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Save the processed data\n",
    "aapl_df.to_csv(\"processed_aapl_data.csv\", index=False)\n",
    "\n",
    "print(\"Feature engineering completed! Processed data saved as 'processed_aapl_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_state(state, window_size=20):\n",
    "    \"\"\"Normalize the input features and apply a 20-day window.\"\"\"\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    state = (state - np.mean(state)) / (np.std(state) + 1e-5)  # Standardization\n",
    "    state = np.concatenate([state[-window_size:], np.zeros(max(0, window_size - len(state)))])\n",
    "    return torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Convert to tensor\n",
    "\n",
    "# Define the Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * 20, 128)  # Adjusted for 20-day window\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, lr=0.001, batch_size=32, memory_size=10000):\n",
    "        self.state_size = state_size * 20  # Adjusted for 20-day window\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        self.model = DQN(self.state_size, action_size)\n",
    "        self.target_model = DQN(self.state_size, action_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Sync target model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def select_action(self, state, epsilon=0.1):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)  # Random action\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.model(state)).item()  # Best action from Q-network\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using experience replay.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.cat(next_states)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "        \n",
    "        loss = self.criterion(q_values, target_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Sync the target model with the main model.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoreq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
