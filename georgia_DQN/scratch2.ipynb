{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering completed! Processed data saved as 'processed_aapl_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import talib\n",
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "aapl_df = pd.read_csv(\"aapl_data.csv\")\n",
    "\n",
    "# Ensure the Date column is in datetime format\n",
    "if 'Date' in aapl_df.columns:\n",
    "    aapl_df['Date'] = pd.to_datetime(aapl_df['Date'])\n",
    "\n",
    "# Sort by date\n",
    "aapl_df = aapl_df.sort_values(by='Date', ascending=True) if 'Date' in aapl_df.columns else aapl_df\n",
    "\n",
    "### 1. Candlestick Pattern Recognition ###\n",
    "patterns = {\n",
    "    \"Doji\": talib.CDLDOJI,\n",
    "    \"Engulfing\": talib.CDLENGULFING,\n",
    "    \"Hammer\": talib.CDLHAMMER,\n",
    "    \"Morning Star\": talib.CDLMORNINGSTAR,\n",
    "    \"Evening Star\": talib.CDLEVENINGSTAR,\n",
    "}\n",
    "\n",
    "for pattern_name, pattern_func in patterns.items():\n",
    "    aapl_df[pattern_name] = pattern_func(aapl_df['Open'], aapl_df['High'], aapl_df['Low'], aapl_df['Close'])\n",
    "\n",
    "### 2. Technical Indicators ###\n",
    "aapl_df['SMA_10'] = talib.SMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['SMA_50'] = talib.SMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['EMA_10'] = talib.EMA(aapl_df['Close'], timeperiod=10)\n",
    "aapl_df['EMA_50'] = talib.EMA(aapl_df['Close'], timeperiod=50)\n",
    "aapl_df['RSI_14'] = talib.RSI(aapl_df['Close'], timeperiod=14)\n",
    "aapl_df['MACD'], aapl_df['MACD_Signal'], aapl_df['MACD_Hist'] = talib.MACD(aapl_df['Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "# ### 3. Normalization ###\n",
    "# price_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'SMA_10', 'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "# aapl_df[price_columns] = (aapl_df[price_columns] - aapl_df[price_columns].min()) / (aapl_df[price_columns].max() - aapl_df[price_columns].min())\n",
    "\n",
    "# ### 4. Windowed Representation ###\n",
    "# window_size = 3  \n",
    "# feature_columns = ['Close', 'Volume', 'SMA_10', 'SMA_50', 'RSI_14', 'MACD', 'MACD_Signal', 'MACD_Hist']\n",
    "\n",
    "# for col in feature_columns:\n",
    "#     for i in range(1, window_size + 1):\n",
    "#         aapl_df[f\"{col}_lag{i}\"] = aapl_df[col].shift(i)\n",
    "\n",
    "# aapl_df = aapl_df.dropna().reset_index(drop=True)\n",
    "\n",
    "# Save the processed data\n",
    "aapl_df.to_csv(\"processed_aapl_data.csv\", index=False)\n",
    "\n",
    "print(\"Feature engineering completed! Processed data saved as 'processed_aapl_data.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mStockTradingEnv\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mEnv):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, initial_balance=10000, trading_fee=0.001, window_size=10):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        \n",
    "        # Load market data\n",
    "        self.df = df.copy()\n",
    "        self.initial_balance = initial_balance\n",
    "        self.trading_fee = trading_fee\n",
    "        self.window_size = window_size\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        \n",
    "        # Define state space (features from the dataset)\n",
    "        self.feature_columns = [\n",
    "            'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'Doji',\n",
    "            'Engulfing', 'Hammer', 'Morning Star', 'Evening Star', 'SMA_10',\n",
    "            'SMA_50', 'EMA_10', 'EMA_50', 'RSI_14', 'MACD', 'MACD_Signal',\n",
    "            'MACD_Hist'\n",
    "        ]\n",
    "        self.state_size = len(self.feature_columns)\n",
    "        \n",
    "        # Define action space (Buy, Sell, Hold)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.state_size,), dtype=np.float32)\n",
    "        \n",
    "        # Portfolio state variables\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = initial_balance\n",
    "        self.next_portfolio_value = initial_balance\n",
    "        self.returns = []\n",
    "        self.actions = []\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.done = False\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.returns = []\n",
    "        \n",
    "        return self._next_observation()\n",
    "    \n",
    "    def _next_observation(self):\n",
    "        \"\"\"Returns the current market state as a feature vector.\"\"\"\n",
    "        return np.array(self.df.iloc[self.current_step][[col for col in self.feature_columns]], dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Executes the given action and moves the environment forward.\"\"\"\n",
    "        current_price = self.df.iloc[self.current_step]['Close']\n",
    "        next_price = self.df.iloc[self.current_step + 1]['Close']\n",
    "        \n",
    "        if action == 1:  # Buy\n",
    "            if self.balance > 0.001:\n",
    "                shares_to_buy = self.balance / (current_price * (1 + self.trading_fee))\n",
    "                self.shares_held = shares_to_buy\n",
    "                self.balance -= shares_to_buy * current_price * (1 + self.trading_fee)\n",
    "        elif action == 2:  # Sell\n",
    "            if self.shares_held > 0.001:\n",
    "                self.balance += self.shares_held * current_price * (1 - self.trading_fee)\n",
    "                self.shares_held = 0\n",
    "        \n",
    "        self.portfolio_value = self.balance + (self.shares_held * current_price)\n",
    "        self.next_portfolio_value = self.balance + (self.shares_held * next_price)\n",
    "        \n",
    "        # Compute returns\n",
    "        self.returns.append((self.next_portfolio_value - self.portfolio_value) / self.portfolio_value)\n",
    "        self.actions.append(action)\n",
    "        \n",
    "        # Compute Sharpe Ratio (risk-adjusted reward)\n",
    "        if len(self.returns) > 1:\n",
    "            mean_return = np.mean(self.returns)\n",
    "            std_return = np.std(self.returns) if np.std(self.returns) > 0 else 1\n",
    "            sharpe_ratio = mean_return / std_return\n",
    "        else:\n",
    "            sharpe_ratio = 0\n",
    "        \n",
    "        reward = sharpe_ratio\n",
    "        \n",
    "        self.portfolio_value = self.next_portfolio_value\n",
    "\n",
    "        # Move to the next step\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            self.done = True\n",
    "        \n",
    "        return self._next_observation(), reward, self.done, {}\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Displays the current portfolio state.\"\"\"\n",
    "        print(f'Step: {self.current_step}, Balance: {self.balance:.2f}, Shares Held: {self.shares_held}, Portfolio Value: {self.portfolio_value}, Sharpe Ratio: {reward}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_state(state, window_size=20):\n",
    "    \"\"\"Normalize the input features and apply a 20-day window.\"\"\"\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    state = (state - np.mean(state)) / (np.std(state) + 1e-5)  # Standardization\n",
    "    state = np.concatenate([state[-window_size:], np.zeros(max(0, window_size - len(state)))])\n",
    "    return torch.tensor(state, dtype=torch.float32).unsqueeze(0)  # Convert to tensor\n",
    "\n",
    "# Define the Deep Q-Network (DQN)\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim * 20, 128)  # Adjusted for 20-day window\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)  # Q-values for each action\n",
    "\n",
    "# Define the DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.90, lr=0.001, batch_size=32, memory_size=10000):\n",
    "        self.state_size = state_size * 20  # Adjusted for 20-day window\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "        self.model = DQN(self.state_size, action_size)\n",
    "        self.target_model = DQN(self.state_size, action_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())  # Sync target model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "    def select_action(self, state, epsilon=0.1):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_size - 1)  # Random action\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(self.model(state)).item()  # Best action from Q-network\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent using experience replay.\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(1)\n",
    "        next_states = torch.cat(next_states)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "        q_values = self.model(states).gather(1, actions)\n",
    "        next_q_values = self.target_model(next_states).max(1)[0].unsqueeze(1)\n",
    "        target_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
    "        \n",
    "        loss = self.criterion(q_values, target_q_values.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        \"\"\"Sync the target model with the main model.\"\"\"\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoreq312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
