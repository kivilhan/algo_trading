{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_ops\n",
    "import data_pipes\n",
    "import constants\n",
    "import yfinance as yf\n",
    "\n",
    "# Get symbols from constants file\n",
    "lines = constants.sa_str.splitlines()\n",
    "symbols = [line.split(\"\\t\")[1] for line in lines][:100]\n",
    "\n",
    "# Download data\n",
    "df = yf.download(symbols + [\"SPY\"], period=\"1y\", interval=\"1h\", ignore_tz=True)\n",
    "\n",
    "data = data_pipes.nova_data(symbols, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "trim_ratio = 0.99\n",
    "split_ratio = 0.999\n",
    "\n",
    "x_base = data[\"x\"][int(trim_ratio * len(data[\"x\"])):]\n",
    "y_base = data[\"y\"][int(trim_ratio * len(data[\"y\"])):]\n",
    "\n",
    "x_t_np = x_base[:int(split_ratio * len(x_base))]\n",
    "x_v_np = x_base[int(split_ratio * len(x_base)):]\n",
    "y_t_np = y_base[:int(split_ratio * len(y_base))]\n",
    "y_v_np = y_base[int(split_ratio * len(y_base)):]\n",
    "\n",
    "x_t = torch.tensor(x_t_np, dtype=torch.float32)\n",
    "x_v = torch.tensor(x_v_np, dtype=torch.float32)\n",
    "y_t = torch.tensor(y_t_np, dtype=torch.float32)\n",
    "y_v = torch.tensor(y_v_np, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from copy import deepcopy\n",
    "\n",
    "class nova_0(nn.Module):\n",
    "    def __init__(self, num_symbols, reduction_factor=1):\n",
    "        super().__init__()\n",
    "        self.hidden0 = nn.Linear(num_symbols, num_symbols // reduction_factor)\n",
    "        self.act0 = nn.SELU()\n",
    "        self.hidden1 = nn.Linear(num_symbols // reduction_factor, num_symbols // reduction_factor)\n",
    "        self.act1 = nn.SELU()\n",
    "        self.hidden2 = nn.Linear(num_symbols // reduction_factor, num_symbols // reduction_factor)\n",
    "        self.act2 = nn.SELU()\n",
    "        self.hidden3 = nn.Linear(num_symbols // reduction_factor, num_symbols)\n",
    "        self.out = nn.Tanh()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act0(self.hidden0(x))\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        # x = self.act2(self.hidden2(x))\n",
    "        x = self.out(self.hidden3(x))\n",
    "        # x = self.hidden3(x)\n",
    "        return x\n",
    "\n",
    "nova = nova_0(x_t.shape[1])\n",
    "epochs, batch_size, lr = 1000, x_t.shape[0]//2, 5e-5\n",
    "optimizer = optim.NAdam(nova.parameters(), lr=lr)\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "test_loss = 999\n",
    "for epoch in range(epochs):\n",
    "    loss = torch_ops.train_loop(x_t, y_t, nova, loss_fn, optimizer, batch_size)\n",
    "    test_loss_new = torch_ops.test_loop(x_v, y_v, nova, loss_fn, batch_size)\n",
    "    if test_loss_new < test_loss or epoch == epochs - 1:\n",
    "        print(f\"---------- Epoch {epoch + 1} ----------\")\n",
    "        print(f\"loss: {loss:1.5f}, test loss: {test_loss_new:1.5f}\")\n",
    "        \n",
    "    if test_loss_new < test_loss:\n",
    "        test_loss = test_loss_new\n",
    "        print(f\"Checkpoint: test loss = {test_loss:1.5f} <---------------\")\n",
    "        state_dict_save = deepcopy(nova.state_dict())\n",
    "\n",
    "print(\"Done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoreq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
